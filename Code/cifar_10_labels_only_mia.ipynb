{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cifar_10_labels_only_mia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vzAuc5Xo3j2"
      },
      "source": [
        "# Label Only Membership Inference (Revisited on points)\n",
        "\n",
        "### Threat Model:\n",
        "\n",
        "- **Black Box** access to an overfitted classifier with no access to actual $D_{train}$\n",
        "- Predict API returns **only labels instead of confidence vectors**\n",
        "- We have some insight on the training data distribution, $D_{out}$ , **but** $D_{train} \\cap D_{out} = \\varnothing$\n",
        "\n",
        "\n",
        "### Attack Target: \n",
        "- Use a shadow model to attack local shadow models and extract membership leakage features\n",
        "- Use data perturbations in order to exploit test/training data approximation relevancies to the classification boundaries.\n",
        "- Perfom the boundary-based attack on the actual model\n",
        "\n",
        "### Evaluation Target\n",
        "- Score over $50\\%$ accuracy\n",
        "- Train attack model based on this assumption and compare with conf-vector attack\n",
        "\n",
        "Implemented based on [this paper](https://arxiv.org/abs/2007.14321)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg_WQJ7j5n3B",
        "outputId": "e3d7b3a3-9875-4164-a3b7-b0e26df471b1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# for image interpolation\n",
        "import scipy.ndimage.interpolation as interpolation\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kv7Qon0qRTk"
      },
      "source": [
        "## Target Model\n",
        "\n",
        "### Model Architecture\n",
        "- 2 layers of 32 $3\\times 3$ Conv2D filters with Max Pooling\n",
        "- 2 layers of 64 $3\\times 3$ Conv2D filters with MaxPooling\n",
        "- Dense Layer of 512 neurons\n",
        "- Dense Output layer of 10 neurons\n",
        "- Each layer has ReLU activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacp4ArauIET"
      },
      "source": [
        "D_TARGET_SIZE = 2500"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qg7LkXX0zOF"
      },
      "source": [
        "def f_target(X_train, y_train, X_test=None, y_test=None, epochs=100):\n",
        "  \"\"\"\n",
        "  Returns a trained target model, if test data are specified we will evaluate the model and print its accuracy\n",
        "  \"\"\"\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(10))\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  if X_test is None or y_test is None:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_split=0.2)\n",
        "  else:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test))\n",
        "  return model"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy2NLipP75sX"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "  # use the rest as testing - 'out' records\n",
        "  attacker_labels = np.concatenate((train_labels[D_TARGET_SIZE:], test_labels))\n",
        "  attacker_images = np.concatenate((train_images[D_TARGET_SIZE:], test_images))\n",
        "\n",
        "  target_images = train_images[:D_TARGET_SIZE]\n",
        "  target_labels = train_labels[:D_TARGET_SIZE]\n"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap02yKRT76RJ",
        "outputId": "48a53ec8-eda1-444e-d167-18f4ba4426f0"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  train_images, eval_images, train_labels, eval_labels = train_test_split(target_images, target_labels, test_size=0.2, shuffle=True)\n",
        "  target_model = f_target(train_images, train_labels, eval_images, eval_labels, epochs=50) "
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 [==============================] - 2s 17ms/step - loss: 4.7242 - accuracy: 0.1270 - val_loss: 2.2428 - val_accuracy: 0.1520\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 2.0672 - accuracy: 0.2605 - val_loss: 2.0127 - val_accuracy: 0.2740\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.7277 - accuracy: 0.3950 - val_loss: 1.9360 - val_accuracy: 0.3400\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.3751 - accuracy: 0.5205 - val_loss: 1.8412 - val_accuracy: 0.3500\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 1.0473 - accuracy: 0.6400 - val_loss: 1.9409 - val_accuracy: 0.3500\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.7166 - accuracy: 0.7660 - val_loss: 2.1337 - val_accuracy: 0.3580\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.4423 - accuracy: 0.8535 - val_loss: 2.4419 - val_accuracy: 0.3840\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.2509 - accuracy: 0.9220 - val_loss: 3.0021 - val_accuracy: 0.3320\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.1790 - accuracy: 0.9515 - val_loss: 3.7982 - val_accuracy: 0.3480\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.1997 - accuracy: 0.9415 - val_loss: 3.4723 - val_accuracy: 0.3380\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.1630 - accuracy: 0.9520 - val_loss: 3.8186 - val_accuracy: 0.3160\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0843 - accuracy: 0.9765 - val_loss: 3.9345 - val_accuracy: 0.3820\n",
            "Epoch 13/50\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0602 - accuracy: 0.9790 - val_loss: 4.4399 - val_accuracy: 0.3520\n",
            "Epoch 14/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0741 - accuracy: 0.9810 - val_loss: 4.1935 - val_accuracy: 0.3520\n",
            "Epoch 15/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0105 - accuracy: 0.9995 - val_loss: 4.2889 - val_accuracy: 0.4000\n",
            "Epoch 16/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.5030 - val_accuracy: 0.3900\n",
            "Epoch 17/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.6042 - val_accuracy: 0.3840\n",
            "Epoch 18/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 5.7120e-04 - accuracy: 1.0000 - val_loss: 4.7011 - val_accuracy: 0.3900\n",
            "Epoch 19/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 4.3946e-04 - accuracy: 1.0000 - val_loss: 4.8026 - val_accuracy: 0.3880\n",
            "Epoch 20/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 3.4896e-04 - accuracy: 1.0000 - val_loss: 4.8728 - val_accuracy: 0.3880\n",
            "Epoch 21/50\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 2.9834e-04 - accuracy: 1.0000 - val_loss: 4.9408 - val_accuracy: 0.3920\n",
            "Epoch 22/50\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 2.5624e-04 - accuracy: 1.0000 - val_loss: 4.9994 - val_accuracy: 0.3900\n",
            "Epoch 23/50\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 2.2242e-04 - accuracy: 1.0000 - val_loss: 5.0568 - val_accuracy: 0.3900\n",
            "Epoch 24/50\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 1.9761e-04 - accuracy: 1.0000 - val_loss: 5.1067 - val_accuracy: 0.3880\n",
            "Epoch 25/50\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.7587e-04 - accuracy: 1.0000 - val_loss: 5.1580 - val_accuracy: 0.3900\n",
            "Epoch 26/50\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 1.5806e-04 - accuracy: 1.0000 - val_loss: 5.2043 - val_accuracy: 0.3900\n",
            "Epoch 27/50\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 1.4315e-04 - accuracy: 1.0000 - val_loss: 5.2434 - val_accuracy: 0.3900\n",
            "Epoch 28/50\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 1.2969e-04 - accuracy: 1.0000 - val_loss: 5.2867 - val_accuracy: 0.3900\n",
            "Epoch 29/50\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 1.1881e-04 - accuracy: 1.0000 - val_loss: 5.3238 - val_accuracy: 0.3900\n",
            "Epoch 30/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 1.0834e-04 - accuracy: 1.0000 - val_loss: 5.3614 - val_accuracy: 0.3920\n",
            "Epoch 31/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 1.0009e-04 - accuracy: 1.0000 - val_loss: 5.3955 - val_accuracy: 0.3920\n",
            "Epoch 32/50\n",
            "63/63 [==============================] - 1s 21ms/step - loss: 9.2173e-05 - accuracy: 1.0000 - val_loss: 5.4336 - val_accuracy: 0.3920\n",
            "Epoch 33/50\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 8.5060e-05 - accuracy: 1.0000 - val_loss: 5.4698 - val_accuracy: 0.3920\n",
            "Epoch 34/50\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 7.8765e-05 - accuracy: 1.0000 - val_loss: 5.5013 - val_accuracy: 0.3940\n",
            "Epoch 35/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 7.4267e-05 - accuracy: 1.0000 - val_loss: 5.5319 - val_accuracy: 0.3940\n",
            "Epoch 36/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 6.8255e-05 - accuracy: 1.0000 - val_loss: 5.5670 - val_accuracy: 0.3940\n",
            "Epoch 37/50\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 6.3812e-05 - accuracy: 1.0000 - val_loss: 5.5974 - val_accuracy: 0.3920\n",
            "Epoch 38/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 5.9429e-05 - accuracy: 1.0000 - val_loss: 5.6206 - val_accuracy: 0.3960\n",
            "Epoch 39/50\n",
            "63/63 [==============================] - 2s 24ms/step - loss: 5.5895e-05 - accuracy: 1.0000 - val_loss: 5.6491 - val_accuracy: 0.3940\n",
            "Epoch 40/50\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 5.2310e-05 - accuracy: 1.0000 - val_loss: 5.6813 - val_accuracy: 0.3920\n",
            "Epoch 41/50\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 4.9078e-05 - accuracy: 1.0000 - val_loss: 5.7075 - val_accuracy: 0.3920\n",
            "Epoch 42/50\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 4.6056e-05 - accuracy: 1.0000 - val_loss: 5.7361 - val_accuracy: 0.3920\n",
            "Epoch 43/50\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 4.3587e-05 - accuracy: 1.0000 - val_loss: 5.7627 - val_accuracy: 0.3900\n",
            "Epoch 44/50\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 4.1107e-05 - accuracy: 1.0000 - val_loss: 5.7923 - val_accuracy: 0.3920\n",
            "Epoch 45/50\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.8733e-05 - accuracy: 1.0000 - val_loss: 5.8146 - val_accuracy: 0.3900\n",
            "Epoch 46/50\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 3.6564e-05 - accuracy: 1.0000 - val_loss: 5.8412 - val_accuracy: 0.3900\n",
            "Epoch 47/50\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 3.4843e-05 - accuracy: 1.0000 - val_loss: 5.8594 - val_accuracy: 0.3900\n",
            "Epoch 48/50\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 3.2797e-05 - accuracy: 1.0000 - val_loss: 5.8857 - val_accuracy: 0.3900\n",
            "Epoch 49/50\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 3.1167e-05 - accuracy: 1.0000 - val_loss: 5.9149 - val_accuracy: 0.3880\n",
            "Epoch 50/50\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.9498e-05 - accuracy: 1.0000 - val_loss: 5.9365 - val_accuracy: 0.3880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9I5lKSrSsU"
      },
      "source": [
        "### Target Model prediction API\n",
        "Provide the users with a prediction API call that returns only the predicted label/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvocFuQeVLsM"
      },
      "source": [
        "# API of model to get predictions : returns labels only\n",
        "def target_predict(model, X):\n",
        "  prob = layers.Softmax()\n",
        "  ret = prob(model.predict(X)).numpy()\n",
        "  return np.apply_along_axis(np.argmax, 1, ret).reshape((-1, 1))"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perturbed Instance Behaviour\n",
        "\n",
        "Following we will apply some perturbations to data instances from in and out of $D_{target}$ and we will count how the predicted label change in respect to this perturbations, according to each class."
      ],
      "metadata": {
        "id": "l--0MCg-elRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "So we can see that the data outside of training set are quite sensitive to perturbations, while the targets training data instances are a little more robust."
      ],
      "metadata": {
        "id": "WMTHfKgHwEVe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H0PYHLFrWJ1"
      },
      "source": [
        "\n",
        "\n",
        "## Shadow Models\n",
        "Following we define our own shadow models\n",
        "\n",
        "### Shadow Model Architecture\n",
        "- 3 CNN layers of $32, 64, 128$ filters of size $3 \\times 3$ with MaxPooling and ReLU activation\n",
        "- Dense Layer of 128 nodes\n",
        "- Dense Layer of 10 nodes as Output layer\n",
        "\n",
        "All output logits pass through Softmax Unit as in the target model to acquire probability vectors\n",
        "\n",
        "\n",
        "\n",
        "### Shadow Dataset Composition\n",
        "\n",
        "We just divide the CIFAR-10 dataset to $D_{out}$ and $D_{train}$ such as $D_{train} \\cap D_{out} = \\varnothing$ and use $D_{out}$ in order to train/test shadow models and attack model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GWyCXmmwIiJ"
      },
      "source": [
        "N_SHADOWS = 5\n",
        "D_SHADOW_SIZE = 1000"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rwhySHfVQjV"
      },
      "source": [
        "def f_shadow(X_train, y_train, X_test=None, y_test=None, epochs=25):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  \n",
        "  \n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(10)   )\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  if X_test is None or y_test is None:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_split=0.2)\n",
        "  else:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test))\n",
        "  return model"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sliyw7xO2PTg"
      },
      "source": [
        "\n",
        "def divide_dataset(n_shadows, shadow_dataset_size, X, y):\n",
        "  D_shadows = []\n",
        "  for i in range(n_shadows):\n",
        "    sample_i = np.random.choice(range(X.shape[0]), shadow_dataset_size, replace=False)\n",
        "    assert np.unique(sample_i).shape[0] == shadow_dataset_size # sanity check\n",
        "    D_shadows.append((X[sample_i, :], y[sample_i, :]))\n",
        "  return D_shadows\n",
        "\n",
        "# returns a list of 'n_shadows' datasets\n",
        "def generate_shadow_dataset(target_model, n_shadows, shadow_dataset_size, n_classes, attacker_X=None, attacker_y=None):\n",
        "  # param target model is not used yet\n",
        "\n",
        "\n",
        "  # in case we give test data we will just divide those to train the shadow models\n",
        "  if attacker_X is not None and attacker_y is not None:\n",
        "    return divide_dataset(n_shadows, shadow_dataset_size, attacker_X, attacker_y)\n",
        "  else:\n",
        "    raise ValueError(\"X and y provided are None.\")\n",
        "\n",
        "# returns list of (trained shadow_model, D_shadow)\n",
        "def create_shadows(D_shadows):\n",
        "  shadow_models = [] # shadow model list\n",
        "\n",
        "  for D_shadow in D_shadows:\n",
        "    # sample data to feed/evaluate the model\n",
        "    X_shadow, y_shadow = D_shadow\n",
        "    shadow_X_train, shadow_X_test, shadow_y_train, shadow_y_test = train_test_split(X_shadow, y_shadow, shuffle=True, test_size=0.33)\n",
        "\n",
        "    # generate the shadow model\n",
        "    shadow_model = f_shadow(shadow_X_train, shadow_y_train, shadow_X_test, shadow_y_test)\n",
        "\n",
        "    D_shadow = (shadow_X_train, shadow_y_train), (shadow_X_test, shadow_y_test)\n",
        "    shadow_models.append((shadow_model, D_shadow))\n",
        "\n",
        "  return shadow_models # return a list where every item is (model, acc), train-data, test-data"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ziH9LP3CY5"
      },
      "source": [
        "# generate shadow datasets\n",
        "D_shadows = generate_shadow_dataset(target_model, N_SHADOWS, D_SHADOW_SIZE, 10, attacker_images, attacker_labels)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dc8morl-NRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6c3e00-0ef0-4ebe-9f58-5fca7b30dc3c"
      },
      "source": [
        "# train the shadow models\n",
        "shadow_models = create_shadows(D_shadows)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 22ms/step - loss: 19.5092 - accuracy: 0.1239 - val_loss: 3.3918 - val_accuracy: 0.1636\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 2.6511 - accuracy: 0.1716 - val_loss: 2.2598 - val_accuracy: 0.2152\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 2.1177 - accuracy: 0.2567 - val_loss: 2.1191 - val_accuracy: 0.2424\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.8631 - accuracy: 0.3343 - val_loss: 1.9302 - val_accuracy: 0.3030\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.6715 - accuracy: 0.4164 - val_loss: 1.9043 - val_accuracy: 0.3364\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.4118 - accuracy: 0.5209 - val_loss: 1.9164 - val_accuracy: 0.3061\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.2205 - accuracy: 0.5791 - val_loss: 1.8404 - val_accuracy: 0.3606\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.9768 - accuracy: 0.6701 - val_loss: 1.9311 - val_accuracy: 0.3485\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.8088 - accuracy: 0.7478 - val_loss: 1.9259 - val_accuracy: 0.3606\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.6357 - accuracy: 0.8239 - val_loss: 2.0861 - val_accuracy: 0.3727\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.4794 - accuracy: 0.8761 - val_loss: 2.1050 - val_accuracy: 0.3545\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.3287 - accuracy: 0.9373 - val_loss: 2.1752 - val_accuracy: 0.4121\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.2277 - accuracy: 0.9597 - val_loss: 2.4019 - val_accuracy: 0.3788\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.1704 - accuracy: 0.9761 - val_loss: 2.2702 - val_accuracy: 0.3788\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 19ms/step - loss: 0.1115 - accuracy: 0.9881 - val_loss: 2.3472 - val_accuracy: 0.4000\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1093 - accuracy: 0.9821 - val_loss: 2.7227 - val_accuracy: 0.3636\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0927 - accuracy: 0.9955 - val_loss: 2.7186 - val_accuracy: 0.3788\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0807 - accuracy: 0.9881 - val_loss: 2.5895 - val_accuracy: 0.3970\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0656 - accuracy: 0.9985 - val_loss: 2.5691 - val_accuracy: 0.4242\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9970 - val_loss: 2.9685 - val_accuracy: 0.3909\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 20ms/step - loss: 0.0761 - accuracy: 0.9851 - val_loss: 2.9071 - val_accuracy: 0.3636\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 20ms/step - loss: 0.1106 - accuracy: 0.9761 - val_loss: 3.0970 - val_accuracy: 0.3758\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 0.1678 - accuracy: 0.9582 - val_loss: 2.8981 - val_accuracy: 0.3758\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1348 - accuracy: 0.9672 - val_loss: 3.0920 - val_accuracy: 0.3606\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0801 - accuracy: 0.9821 - val_loss: 3.0346 - val_accuracy: 0.4091\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 20ms/step - loss: 8.9948 - accuracy: 0.1343 - val_loss: 2.5753 - val_accuracy: 0.1212\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 2.3886 - accuracy: 0.1627 - val_loss: 2.2233 - val_accuracy: 0.1939\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 2.0697 - accuracy: 0.2537 - val_loss: 2.0828 - val_accuracy: 0.2697\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 1.8401 - accuracy: 0.3358 - val_loss: 2.0130 - val_accuracy: 0.2727\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.6273 - accuracy: 0.4254 - val_loss: 1.9013 - val_accuracy: 0.3242\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 1.3714 - accuracy: 0.5328 - val_loss: 2.1458 - val_accuracy: 0.2970\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.2479 - accuracy: 0.5642 - val_loss: 1.9885 - val_accuracy: 0.3394\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0125 - accuracy: 0.6851 - val_loss: 2.1103 - val_accuracy: 0.3121\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.8603 - accuracy: 0.7194 - val_loss: 2.1323 - val_accuracy: 0.3121\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.7413 - accuracy: 0.7806 - val_loss: 2.2088 - val_accuracy: 0.3333\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.6077 - accuracy: 0.8209 - val_loss: 2.3023 - val_accuracy: 0.3576\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.4366 - accuracy: 0.8761 - val_loss: 2.3011 - val_accuracy: 0.3455\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.3165 - accuracy: 0.9254 - val_loss: 2.5554 - val_accuracy: 0.3606\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.2413 - accuracy: 0.9433 - val_loss: 2.6402 - val_accuracy: 0.3424\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.1756 - accuracy: 0.9687 - val_loss: 2.7898 - val_accuracy: 0.3727\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1248 - accuracy: 0.9791 - val_loss: 2.9722 - val_accuracy: 0.3636\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1012 - accuracy: 0.9866 - val_loss: 3.2722 - val_accuracy: 0.3515\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0856 - accuracy: 0.9896 - val_loss: 3.5489 - val_accuracy: 0.3242\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9925 - val_loss: 3.6101 - val_accuracy: 0.3273\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0503 - accuracy: 0.9970 - val_loss: 3.5555 - val_accuracy: 0.3667\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 3.7488 - val_accuracy: 0.3636\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0179 - accuracy: 0.9985 - val_loss: 3.8851 - val_accuracy: 0.3606\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 3.9184 - val_accuracy: 0.3515\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.9956 - val_accuracy: 0.3485\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 4.1126 - val_accuracy: 0.3485\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 22ms/step - loss: 8.2352 - accuracy: 0.1388 - val_loss: 2.4058 - val_accuracy: 0.1848\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 2.1336 - accuracy: 0.2239 - val_loss: 2.1001 - val_accuracy: 0.2697\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.7675 - accuracy: 0.3597 - val_loss: 2.0155 - val_accuracy: 0.2879\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.5878 - accuracy: 0.4478 - val_loss: 2.0342 - val_accuracy: 0.3394\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.3015 - accuracy: 0.5657 - val_loss: 2.0189 - val_accuracy: 0.3485\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0625 - accuracy: 0.6552 - val_loss: 2.1303 - val_accuracy: 0.2909\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.8930 - accuracy: 0.7299 - val_loss: 2.0934 - val_accuracy: 0.3485\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.7011 - accuracy: 0.7940 - val_loss: 2.2341 - val_accuracy: 0.3515\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.5860 - accuracy: 0.8149 - val_loss: 2.1814 - val_accuracy: 0.3667\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.4524 - accuracy: 0.8716 - val_loss: 2.4555 - val_accuracy: 0.3485\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.3193 - accuracy: 0.9358 - val_loss: 2.3909 - val_accuracy: 0.3606\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.2065 - accuracy: 0.9612 - val_loss: 2.6555 - val_accuracy: 0.3576\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.1419 - accuracy: 0.9701 - val_loss: 2.8819 - val_accuracy: 0.3879\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1234 - accuracy: 0.9806 - val_loss: 3.0135 - val_accuracy: 0.3697\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9896 - val_loss: 3.0848 - val_accuracy: 0.3667\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9940 - val_loss: 3.2023 - val_accuracy: 0.3515\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0379 - accuracy: 0.9985 - val_loss: 3.3587 - val_accuracy: 0.3909\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.4322 - val_accuracy: 0.3697\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 3.6877 - val_accuracy: 0.3909\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.7540 - val_accuracy: 0.3697\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.8087 - val_accuracy: 0.4000\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.9161 - val_accuracy: 0.3758\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.8864 - val_accuracy: 0.3788\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.9793 - val_accuracy: 0.3879\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 4.0243 - val_accuracy: 0.3788\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 22ms/step - loss: 19.4694 - accuracy: 0.1179 - val_loss: 2.7607 - val_accuracy: 0.1242\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 2.3948 - accuracy: 0.1821 - val_loss: 2.3143 - val_accuracy: 0.1879\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.9871 - accuracy: 0.2716 - val_loss: 2.2769 - val_accuracy: 0.2242\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.7772 - accuracy: 0.3791 - val_loss: 2.2640 - val_accuracy: 0.1939\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.5505 - accuracy: 0.4582 - val_loss: 2.2265 - val_accuracy: 0.2697\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.2899 - accuracy: 0.5716 - val_loss: 2.1221 - val_accuracy: 0.3212\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.1120 - accuracy: 0.6045 - val_loss: 2.1670 - val_accuracy: 0.3273\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.9129 - accuracy: 0.7045 - val_loss: 2.2128 - val_accuracy: 0.2909\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.7677 - accuracy: 0.7582 - val_loss: 2.2337 - val_accuracy: 0.3273\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.6786 - accuracy: 0.7866 - val_loss: 2.4365 - val_accuracy: 0.3182\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.5595 - accuracy: 0.8328 - val_loss: 2.5075 - val_accuracy: 0.3061\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.4670 - accuracy: 0.8716 - val_loss: 2.5120 - val_accuracy: 0.3455\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.3355 - accuracy: 0.9045 - val_loss: 2.7566 - val_accuracy: 0.3424\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.2341 - accuracy: 0.9493 - val_loss: 2.7797 - val_accuracy: 0.3152\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.1724 - accuracy: 0.9657 - val_loss: 3.0098 - val_accuracy: 0.3364\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1145 - accuracy: 0.9791 - val_loss: 3.0258 - val_accuracy: 0.3303\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0914 - accuracy: 0.9836 - val_loss: 3.5689 - val_accuracy: 0.3182\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 0.0877 - accuracy: 0.9866 - val_loss: 3.1519 - val_accuracy: 0.3515\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0680 - accuracy: 0.9925 - val_loss: 3.3852 - val_accuracy: 0.3485\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0327 - accuracy: 0.9985 - val_loss: 3.4782 - val_accuracy: 0.3394\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0211 - accuracy: 0.9985 - val_loss: 3.6217 - val_accuracy: 0.3485\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.7188 - val_accuracy: 0.3545\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.7658 - val_accuracy: 0.3424\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.8078 - val_accuracy: 0.3485\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.9176 - val_accuracy: 0.3636\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 20ms/step - loss: 15.3942 - accuracy: 0.1328 - val_loss: 2.9766 - val_accuracy: 0.1000\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 2.3147 - accuracy: 0.2045 - val_loss: 2.2556 - val_accuracy: 0.1636\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.9901 - accuracy: 0.2866 - val_loss: 2.3235 - val_accuracy: 0.1848\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.7686 - accuracy: 0.3761 - val_loss: 2.1967 - val_accuracy: 0.2091\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.5732 - accuracy: 0.4493 - val_loss: 2.1345 - val_accuracy: 0.2273\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.4563 - accuracy: 0.5119 - val_loss: 2.3360 - val_accuracy: 0.2091\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.2230 - accuracy: 0.5866 - val_loss: 2.1897 - val_accuracy: 0.2788\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.0244 - accuracy: 0.6657 - val_loss: 2.3853 - val_accuracy: 0.2606\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.8389 - accuracy: 0.7403 - val_loss: 2.5522 - val_accuracy: 0.2545\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.6876 - accuracy: 0.7791 - val_loss: 2.3360 - val_accuracy: 0.3000\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.5007 - accuracy: 0.8627 - val_loss: 2.4026 - val_accuracy: 0.2939\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.4125 - accuracy: 0.8866 - val_loss: 2.5556 - val_accuracy: 0.3242\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.3088 - accuracy: 0.9254 - val_loss: 3.0574 - val_accuracy: 0.3061\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.2788 - accuracy: 0.9284 - val_loss: 3.1245 - val_accuracy: 0.3242\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.2433 - accuracy: 0.9403 - val_loss: 2.9230 - val_accuracy: 0.3455\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.2215 - accuracy: 0.9522 - val_loss: 2.8518 - val_accuracy: 0.3939\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9716 - val_loss: 3.4792 - val_accuracy: 0.3364\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.1034 - accuracy: 0.9821 - val_loss: 3.1011 - val_accuracy: 0.3515\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.0863 - accuracy: 0.9851 - val_loss: 3.2737 - val_accuracy: 0.3455\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9940 - val_loss: 3.6684 - val_accuracy: 0.3152\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0306 - accuracy: 0.9985 - val_loss: 3.5314 - val_accuracy: 0.3333\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 3.6942 - val_accuracy: 0.3545\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.7773 - val_accuracy: 0.3758\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 3.8898 - val_accuracy: 0.3758\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.9105 - val_accuracy: 0.3606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJQxvjrfrfnW"
      },
      "source": [
        "## Attack Model\n",
        "\n",
        "### Attack Model Architecture\n",
        "The attack model is consisted of 1 swallow layer of 10 neurons just as proposed in Shokri et al. and in the relative label only attack paper.\n",
        "\n",
        "\n",
        "### Attack Dataset\n",
        "The attack dataset will be consisted of vectors $x_i$, s.t. $x_i$ contains:\n",
        "- real label\n",
        "- predicted label\n",
        "- bitstring of length $n'$, where $x_{ij+2}, \\; j \\in \\{1, ..., n'\\} $ will be 1 if perturbed label is same as predicted, otherwise it'll be zero.\n",
        "\n",
        "\n",
        "### Perturbed Queries for feature extraction and Attack Dataset\n",
        "\n",
        "In order to construct the actual attack dataset we have 2 perturbation functions:\n",
        "- Translate\n",
        "- Rotate\n",
        "\n",
        "that can apply the necessary augmentations in order to acquire the feature vector for a query.\n",
        "\n",
        "This works by applying all augmentations to the input X and querying the target model in order to return a binary vector $x_{attack}$ where $$x_{attack_p} = 1 \\; if \\;y_p == y_{true} \\; else \\; 0, \\forall p \\in Perturbations(X)$$\n",
        "\n",
        "where $y_p$ is the label for pertubation $p$ of input $X$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhUttNV1jR6"
      },
      "source": [
        "r = 2 # rotate range => creating 2*r+1 rotations \n",
        "d = 1 # translate range =? creating 4*d + 1 translates"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYRgNm8sqkY7"
      },
      "source": [
        "# create all relative rotates for interpolation (returns 2*r + 1 translates)\n",
        "def create_rotates(r):\n",
        "  if r is None:\n",
        "    return None\n",
        "  if r == 1:\n",
        "    return [0.0]\n",
        "  rotates = np.linspace(-r, r, (r * 2 + 1))\n",
        "  return rotates\n",
        "\n",
        "# create all possible translates (returns 4*d+1 translates)\n",
        "def create_translates(d):\n",
        "  if d is None:\n",
        "    return None\n",
        "\n",
        "  def all_shifts(mshift):\n",
        "    if mshift == 0:\n",
        "      return [(0, 0, 0, 0)]\n",
        "    \n",
        "    all_pairs = []\n",
        "    start = (0, mshift, 0, 0)\n",
        "    end = (0, mshift, 0, 0)\n",
        "    vdir = -1\n",
        "    hdir = -1\n",
        "    first_time = True\n",
        "    while (start[1] != end[1] or start[2] != end[2]) or first_time:\n",
        "      all_pairs.append(start)\n",
        "      start = (0, start[1] + vdir, start[2] + hdir, 0)\n",
        "      if abs(start[1]) == mshift:\n",
        "        vdir *= -1\n",
        "      if abs(start[2]) == mshift:\n",
        "        hdir *= -1\n",
        "      first_time = False\n",
        "    all_pairs = [(0, 0, 0, 0)] + all_pairs  # add no shift\n",
        "    return all_pairs\n",
        "\n",
        "  translates = all_shifts(d)\n",
        "  return translates\n",
        "\n",
        "\n",
        "def apply_augment(d, augment, type_):\n",
        "  if type_ == 'd':\n",
        "    d = interpolation.shift(d, augment, mode='constant')\n",
        "  elif type_ == 'r':\n",
        "    d = interpolation.rotate(d, augment, (1, 2), reshape=False)\n",
        "  else:\n",
        "    raise ValueError(f'Augmentation Type: \\'{type_}\\' doesn\\'t exist. Try \\'r\\' or \\'d\\'')\n",
        "  return d\n",
        "\n",
        "# param model the model to query\n",
        "# param X the input to perurb\n",
        "# param y_pred is the predictions of the model for given input\n",
        "def augmented_queries(model, X, y_pred, r=3, d=1):\n",
        "  #create perturbations\n",
        "  rotates = create_rotates(r)\n",
        "  translates = create_translates(d)\n",
        "\n",
        "  X_attack = None\n",
        "  for rot in rotates:\n",
        "    #  create perturbed image\n",
        "    X_perturbed = apply_augment(X, rot, 'r')\n",
        "    # return query line\n",
        "    y_perturbed = target_predict(model, X_perturbed)\n",
        "    X_attack_col = y_perturbed #(y_pred == y_perturbed).astype(int) # transform the prediction column into a binary collumn where x_i = 1 when y_true == y_pred else 0\n",
        "    \n",
        "    if X_attack is None:\n",
        "      X_attack = X_attack_col\n",
        "    else:\n",
        "      X_attack = np.concatenate((X_attack, X_attack_col), axis=1)\n",
        "\n",
        "  for tra in translates:\n",
        "    X_perturbed = apply_augment(X, tra, 'd')\n",
        "    # return query line\n",
        "    y_perturbed = target_predict(model, X_perturbed)\n",
        "    X_attack_col = y_perturbed #(y_pred == y_perturbed).astype(int) # transform the prediction column into a binary collumn where x_i = 1 when y_true == y_pred else 0\n",
        "    # concate the col to the rest of x_attack feature vector\n",
        "    if X_attack is None:\n",
        "      X_attack = X_attack_col\n",
        "    else:\n",
        "      X_attack = np.concatenate((X_attack, X_attack_col), axis=1)\n",
        "  return X_attack"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6lVDDuD2DA5"
      },
      "source": [
        "# helper function to prepare each shadow dataset batch\n",
        "def prepare_batch(model, X, y, in_D):\n",
        "  #decide membership\n",
        "  y_member = np.ones(shape=(y.shape[0], 1)) if in_D else np.zeros(shape=(y.shape[0], 1))\n",
        "\n",
        "  # get the y_pred \n",
        "  prob = layers.Softmax()\n",
        "  ret = prob(model.predict(X)).numpy()\n",
        "  y_pred = np.apply_along_axis(np.argmax, 1, ret).reshape((-1, 1))\n",
        "  perturbed_queries_res = augmented_queries(model, X, y_pred, r, d)\n",
        "  \n",
        "  # return an instance <actual class, predicted class, perturbed_queries_res from shadow models, 'in'/'out' D_target membership> \n",
        "  return np.concatenate((y.reshape(-1, 1), y_pred, perturbed_queries_res, y_member), axis=1)\n",
        "\n",
        "def generate_attack_dataset(shadow_models, n_classes):\n",
        "  # input is a list where items are model, (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "  D_attack = None\n",
        "  # D_attack_i format = <class, prob_vec, membership label (1 or 0)> \n",
        "  for shadow_model, ((X_train, y_train), (X_test, y_test)) in shadow_models:\n",
        "    s = min(X_train.shape[0], X_test.shape[0])\n",
        "    print(f\"Preparing shadow batch of size {2*s}\")\n",
        "    batch = np.concatenate((\n",
        "        prepare_batch(shadow_model, X_train[:s], y_train[:s], True), # members of shadow dataset \n",
        "        prepare_batch(shadow_model, X_test[:s], y_test[:s], False)   # non members of shadow dataset\n",
        "    ))   \n",
        "\n",
        "    D_attack = np.concatenate((D_attack, batch)) if D_attack is not None else batch  \n",
        "    print(\"Done!\")\n",
        "  return D_attack "
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh5MsDOgVT4V"
      },
      "source": [
        "def __f_attack(X_train, y_train, X_test, y_test, epochs=100):\n",
        "  print(X_train.shape, X_test.shape)\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(10, input_shape=(X_train.shape[1],)))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train, epochs=epochs,\n",
        "                    validation_data=(X_test, y_test), verbose=True)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def f_attack(X, y):\n",
        "  # X_i = (class, probability vector, )\n",
        "  classes = np.unique(train_labels) # all class labels\n",
        "  with tf.device('/gpu:0'):\n",
        "  # split to train and test datasets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3)\n",
        "    attack_model = __f_attack(X_train, y_train, X_test, y_test)\n",
        "\n",
        "  return attack_model"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_yfm1hK-Sg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55a4ab1-610b-4b9a-a402-d7c12092a617"
      },
      "source": [
        "D_attack = generate_attack_dataset(shadow_models, 10)\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhE29HBBYGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f81096c-7204-4ced-f465-459f8a36fd7f"
      },
      "source": [
        "attack_model_bundle = f_attack(D_attack[:, :-1], D_attack[:, -1])"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2310, 12) (990, 12)\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 1s 9ms/step - loss: 3.1017 - accuracy: 0.4978 - val_loss: 0.9205 - val_accuracy: 0.5535\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.7945 - accuracy: 0.4745 - val_loss: 0.7539 - val_accuracy: 0.4929\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.7586 - accuracy: 0.5355 - val_loss: 0.7372 - val_accuracy: 0.5323\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.7387 - accuracy: 0.5268 - val_loss: 0.7232 - val_accuracy: 0.5414\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.7244 - accuracy: 0.5303 - val_loss: 0.7124 - val_accuracy: 0.5364\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.7127 - accuracy: 0.5403 - val_loss: 0.7003 - val_accuracy: 0.5081\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.5407 - val_loss: 0.6901 - val_accuracy: 0.4980\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6883 - accuracy: 0.5316 - val_loss: 0.6883 - val_accuracy: 0.5939\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6768 - accuracy: 0.5788 - val_loss: 0.6712 - val_accuracy: 0.5525\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6656 - accuracy: 0.5684 - val_loss: 0.6638 - val_accuracy: 0.5616\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.6573 - accuracy: 0.6000 - val_loss: 0.6596 - val_accuracy: 0.6010\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.6495 - accuracy: 0.6017 - val_loss: 0.6481 - val_accuracy: 0.6394\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6407 - accuracy: 0.6255 - val_loss: 0.6410 - val_accuracy: 0.6434\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.6334 - accuracy: 0.6506 - val_loss: 0.6349 - val_accuracy: 0.6495\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6270 - accuracy: 0.6628 - val_loss: 0.6357 - val_accuracy: 0.6414\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6193 - accuracy: 0.6697 - val_loss: 0.6279 - val_accuracy: 0.6616\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.6103 - accuracy: 0.7147 - val_loss: 0.6142 - val_accuracy: 0.7030\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6021 - accuracy: 0.7329 - val_loss: 0.6103 - val_accuracy: 0.7121\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5946 - accuracy: 0.7338 - val_loss: 0.6024 - val_accuracy: 0.7162\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5887 - accuracy: 0.7420 - val_loss: 0.6004 - val_accuracy: 0.7535\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5812 - accuracy: 0.7398 - val_loss: 0.5872 - val_accuracy: 0.7444\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5759 - accuracy: 0.7472 - val_loss: 0.5815 - val_accuracy: 0.7545\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7671 - val_loss: 0.5760 - val_accuracy: 0.7444\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.7636 - val_loss: 0.5700 - val_accuracy: 0.7566\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5564 - accuracy: 0.7701 - val_loss: 0.5621 - val_accuracy: 0.7586\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.5500 - accuracy: 0.7762 - val_loss: 0.5602 - val_accuracy: 0.7596\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5451 - accuracy: 0.7740 - val_loss: 0.5525 - val_accuracy: 0.7758\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5389 - accuracy: 0.7810 - val_loss: 0.5447 - val_accuracy: 0.7697\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5350 - accuracy: 0.7831 - val_loss: 0.5412 - val_accuracy: 0.7727\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5296 - accuracy: 0.7797 - val_loss: 0.5559 - val_accuracy: 0.7414\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.5243 - accuracy: 0.7840 - val_loss: 0.5314 - val_accuracy: 0.7899\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.5189 - accuracy: 0.7935 - val_loss: 0.5251 - val_accuracy: 0.7818\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5140 - accuracy: 0.7926 - val_loss: 0.5229 - val_accuracy: 0.7859\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7957 - val_loss: 0.5157 - val_accuracy: 0.7869\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.5033 - accuracy: 0.7931 - val_loss: 0.5124 - val_accuracy: 0.7869\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7996 - val_loss: 0.5072 - val_accuracy: 0.7889\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4973 - accuracy: 0.7991 - val_loss: 0.5052 - val_accuracy: 0.7949\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4911 - accuracy: 0.8009 - val_loss: 0.4981 - val_accuracy: 0.8091\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4912 - accuracy: 0.8030 - val_loss: 0.4973 - val_accuracy: 0.7949\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.8074 - val_loss: 0.4901 - val_accuracy: 0.8101\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4817 - accuracy: 0.8078 - val_loss: 0.5096 - val_accuracy: 0.7707\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4789 - accuracy: 0.8061 - val_loss: 0.4821 - val_accuracy: 0.8141\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4741 - accuracy: 0.8095 - val_loss: 0.4802 - val_accuracy: 0.8182\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4740 - accuracy: 0.8108 - val_loss: 0.4770 - val_accuracy: 0.8071\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4693 - accuracy: 0.8087 - val_loss: 0.4754 - val_accuracy: 0.8202\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4670 - accuracy: 0.8177 - val_loss: 0.4691 - val_accuracy: 0.8172\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4638 - accuracy: 0.8095 - val_loss: 0.4725 - val_accuracy: 0.8283\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4629 - accuracy: 0.8143 - val_loss: 0.4736 - val_accuracy: 0.7990\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4606 - accuracy: 0.8035 - val_loss: 0.4676 - val_accuracy: 0.8303\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4563 - accuracy: 0.8165 - val_loss: 0.4614 - val_accuracy: 0.8131\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4521 - accuracy: 0.8216 - val_loss: 0.4593 - val_accuracy: 0.8283\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4520 - accuracy: 0.8165 - val_loss: 0.4535 - val_accuracy: 0.8202\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4478 - accuracy: 0.8212 - val_loss: 0.4547 - val_accuracy: 0.8273\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.8273 - val_loss: 0.4585 - val_accuracy: 0.8030\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4519 - accuracy: 0.8152 - val_loss: 0.4487 - val_accuracy: 0.8182\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4416 - accuracy: 0.8229 - val_loss: 0.4541 - val_accuracy: 0.8354\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4429 - accuracy: 0.8268 - val_loss: 0.4496 - val_accuracy: 0.8141\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4382 - accuracy: 0.8264 - val_loss: 0.4414 - val_accuracy: 0.8222\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4360 - accuracy: 0.8251 - val_loss: 0.4404 - val_accuracy: 0.8222\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4352 - accuracy: 0.8290 - val_loss: 0.4436 - val_accuracy: 0.8172\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4339 - accuracy: 0.8342 - val_loss: 0.4697 - val_accuracy: 0.7909\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.8277 - val_loss: 0.4393 - val_accuracy: 0.8384\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.8312 - val_loss: 0.4407 - val_accuracy: 0.8162\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4293 - accuracy: 0.8286 - val_loss: 0.4331 - val_accuracy: 0.8343\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4260 - accuracy: 0.8290 - val_loss: 0.4355 - val_accuracy: 0.8202\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4288 - accuracy: 0.8290 - val_loss: 0.4333 - val_accuracy: 0.8364\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4320 - accuracy: 0.8156 - val_loss: 0.4504 - val_accuracy: 0.7990\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4317 - accuracy: 0.8238 - val_loss: 0.4253 - val_accuracy: 0.8354\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4208 - accuracy: 0.8342 - val_loss: 0.4253 - val_accuracy: 0.8333\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4200 - accuracy: 0.8368 - val_loss: 0.4481 - val_accuracy: 0.8040\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4214 - accuracy: 0.8320 - val_loss: 0.4226 - val_accuracy: 0.8283\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4198 - accuracy: 0.8329 - val_loss: 0.4218 - val_accuracy: 0.8303\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4179 - accuracy: 0.8342 - val_loss: 0.4327 - val_accuracy: 0.8374\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4193 - accuracy: 0.8359 - val_loss: 0.4285 - val_accuracy: 0.8222\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4183 - accuracy: 0.8307 - val_loss: 0.4202 - val_accuracy: 0.8253\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4159 - accuracy: 0.8338 - val_loss: 0.4176 - val_accuracy: 0.8293\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4182 - accuracy: 0.8312 - val_loss: 0.4213 - val_accuracy: 0.8263\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4155 - accuracy: 0.8307 - val_loss: 0.4174 - val_accuracy: 0.8293\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4167 - accuracy: 0.8364 - val_loss: 0.4230 - val_accuracy: 0.8212\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4170 - accuracy: 0.8346 - val_loss: 0.4167 - val_accuracy: 0.8364\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4098 - accuracy: 0.8338 - val_loss: 0.4128 - val_accuracy: 0.8374\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 0s 7ms/step - loss: 0.4100 - accuracy: 0.8377 - val_loss: 0.4216 - val_accuracy: 0.8242\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4094 - accuracy: 0.8338 - val_loss: 0.4132 - val_accuracy: 0.8364\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4105 - accuracy: 0.8346 - val_loss: 0.4137 - val_accuracy: 0.8303\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4113 - accuracy: 0.8333 - val_loss: 0.4127 - val_accuracy: 0.8333\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4036 - accuracy: 0.8364 - val_loss: 0.4093 - val_accuracy: 0.8313\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 1s 7ms/step - loss: 0.4074 - accuracy: 0.8355 - val_loss: 0.4138 - val_accuracy: 0.8394\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4077 - accuracy: 0.8320 - val_loss: 0.4154 - val_accuracy: 0.8414\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4046 - accuracy: 0.8351 - val_loss: 0.4050 - val_accuracy: 0.8374\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4003 - accuracy: 0.8377 - val_loss: 0.4120 - val_accuracy: 0.8293\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4026 - accuracy: 0.8329 - val_loss: 0.4098 - val_accuracy: 0.8283\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4032 - accuracy: 0.8368 - val_loss: 0.4072 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.3998 - accuracy: 0.8368 - val_loss: 0.4053 - val_accuracy: 0.8394\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4009 - accuracy: 0.8385 - val_loss: 0.4068 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.3987 - accuracy: 0.8381 - val_loss: 0.4051 - val_accuracy: 0.8333\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8364 - val_loss: 0.4083 - val_accuracy: 0.8444\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.3989 - accuracy: 0.8364 - val_loss: 0.4027 - val_accuracy: 0.8414\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.3982 - accuracy: 0.8381 - val_loss: 0.4015 - val_accuracy: 0.8414\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.8377 - val_loss: 0.4000 - val_accuracy: 0.8414\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.3987 - accuracy: 0.8359 - val_loss: 0.4092 - val_accuracy: 0.8263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTAveNFNrvrX"
      },
      "source": [
        "## Attack Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9GxZE5Yntpw"
      },
      "source": [
        "def evaluate_attack(attack_model, X_attack, y_attack, n_classes):\n",
        "  acc_per_class = []\n",
        "  for c in range(n_classes):\n",
        "    class_instances = X_attack[:, 0] == c # get same class samples\n",
        "    test_loss, test_acc = attack_model.evaluate(X_attack[class_instances, :], y_attack[class_instances], verbose=0)\n",
        "    acc_per_class.append(test_acc)\n",
        "    print(f\"class-{c+1}: {test_acc}\")\n",
        "  return acc_per_class"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQyYdB7SABg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415aa318-4592-4a84-a771-cf847e4eca4c"
      },
      "source": [
        "# create a test dataset \n",
        "\n",
        "D_in = prepare_batch(target_model, train_images[:1000], train_labels[:1000], True)\n",
        "print(\"Testing with 'in' data only:\")\n",
        "res_in = evaluate_attack(attack_model_bundle, D_in[:, :-1], D_in[:, -1], 10)\n",
        "\n",
        "D_out = prepare_batch(target_model, attacker_images[:1000], attacker_labels[:1000], False)\n",
        "print(\"\\nTesting with 'out' data only:\")\n",
        "res_out = evaluate_attack(attack_model_bundle, D_out[:, :-1], D_out[:, -1], 10)\n",
        "\n",
        "print(\"\\nTesting with all prev data: \")\n",
        "res_all = evaluate_attack(attack_model_bundle, np.concatenate((D_out[:, :-1], D_in[:, :-1])), np.concatenate((D_out[:, -1], D_in[:, -1])), 10)\n",
        "\n",
        "print(f\"\\nTotal attack accuracy: {np.mean(res_all)}\")"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with 'in' data only:\n",
            "class-1: 1.0\n",
            "class-2: 1.0\n",
            "class-3: 1.0\n",
            "class-4: 1.0\n",
            "class-5: 1.0\n",
            "class-6: 1.0\n",
            "class-7: 1.0\n",
            "class-8: 1.0\n",
            "class-9: 1.0\n",
            "class-10: 1.0\n",
            "\n",
            "Testing with 'out' data only:\n",
            "class-1: 0.5684210658073425\n",
            "class-2: 0.488095223903656\n",
            "class-3: 0.7886179089546204\n",
            "class-4: 0.7849462628364563\n",
            "class-5: 0.6875\n",
            "class-6: 0.6126126050949097\n",
            "class-7: 0.4423076808452606\n",
            "class-8: 0.5113636255264282\n",
            "class-9: 0.6132075190544128\n",
            "class-10: 0.5299999713897705\n",
            "\n",
            "Testing with all prev data: \n",
            "class-1: 0.7929292917251587\n",
            "class-2: 0.7724867463111877\n",
            "class-3: 0.8818181753158569\n",
            "class-4: 0.88165682554245\n",
            "class-5: 0.8598130941390991\n",
            "class-6: 0.7971698045730591\n",
            "class-7: 0.7170731425285339\n",
            "class-8: 0.7724867463111877\n",
            "class-9: 0.800000011920929\n",
            "class-10: 0.7638190984725952\n",
            "\n",
            "Total attack accuracy: 0.8039252936840058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (attack_model_bundle.predict(np.concatenate((D_out[:, :-1], D_in[:, :-1]))) > 0.5).astype(np.int8)\n",
        "y_true = np.concatenate((D_out[:, -1], D_in[:, -1]))\n",
        "print(classification_report(y_true.reshape(-1), y_pred.reshape(-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDNdLqnnyVp",
        "outputId": "cf4e137f-ebf5-4872-87bc-c9d894fbda44"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.61      0.76      1000\n",
            "         1.0       0.72      1.00      0.84      1000\n",
            "\n",
            "    accuracy                           0.80      2000\n",
            "   macro avg       0.86      0.80      0.80      2000\n",
            "weighted avg       0.86      0.80      0.80      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = attack_model_bundle.predict(np.concatenate((D_out[:, :-1], D_in[:, :-1])))\n",
        "y_true = np.concatenate((D_out[:, -1], D_in[:, -1]))\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "\n",
        "print(f\"AUC = {roc_auc_score(y_true, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G8aQH6Nqp3Ah",
        "outputId": "28a0c9f6-5328-40ad-81aa-587fa019a9e7"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC = 0.8110490000000001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8grjprA4PmE7"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check target model's behaviour in perturbed images"
      ],
      "metadata": {
        "id": "w9g2jus3rP1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def study_perturbations(model, X, y, rs, ts):\n",
        "  diffs = []\n",
        "  y_pred = target_predict(model, X)    \n",
        "  for c in range(10):\n",
        "    #  given class acquire the changes in perturbed input instances given the model\n",
        "    idx = y_pred[:, 0] == c\n",
        "    X_c = X[idx]\n",
        "    y_pred_c = y_pred[idx]\n",
        "    perturbed_labels = (augmented_queries(model, X_c, y_pred_c, rs, ts) == y_pred_c).astype(np.int8)\n",
        "    # Now we have to count how many labels diverge from the predicted label\n",
        "    diff = len(perturbed_labels.reshape(-1)) - sum(perturbed_labels.reshape(-1)) # the labels are binary where 1 == y_pred = y_perturbed, otherwise 0\n",
        "    diffs.append(int(100 * diff/len(perturbed_labels.reshape(-1)))) # append the percentage of changes in the class sample\n",
        "    \n",
        "  return diffs "
      ],
      "metadata": {
        "id": "sD5YbfusfCob"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_SAMPLES = 100\n",
        "train_idx = np.random.choice(range(train_images.shape[0]), N_SAMPLES, replace=False)\n",
        "test_idx = np.random.choice(range(attacker_images.shape[0]), N_SAMPLES, replace=False)"
      ],
      "metadata": {
        "id": "w-0sJVhfbfQ3"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# tests in D_in\n",
        "diffs_per_class_D_in = study_perturbations(target_model, train_images[train_idx], train_labels[train_idx], r, d) \n",
        "total_diffs_in = sum(diffs_per_class_D_in)/10\n",
        "axes[0].set_title('D_in predicted label divergence');\n",
        "axes[0].barh(list(range(10)), diffs_per_class_D_in, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "# tests in D_out  \n",
        "diffs_per_class_D_out= study_perturbations(target_model,attacker_images[test_idx],attacker_labels[test_idx], r, d)\n",
        "total_diffs_out = sum(diffs_per_class_D_out)/10\n",
        "axes[1].set_title('D_out predicted label divergence');\n",
        "axes[1].barh(list(range(10)), diffs_per_class_D_out, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "axes[2].set_title('Total predicted label divergence percentage');\n",
        "axes[2].barh([1, 0], [total_diffs_in, total_diffs_out], tick_label=['In', 'Out'])\n",
        "\n",
        "plt.setp(axes, xticks=range(0, 101, 10), xticklabels=[f'{i}%' for i in range(0, 101, 10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dpYCPdS0EA-p",
        "outputId": "7f267118-5d81-4ba3-b03f-b34a28589c87"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.axis.XTick at 0x7f81dd74fa10>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd74f250>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd74f150>,\n",
              " <matplotlib.axis.XTick at 0x7f8353ee4390>,\n",
              " <matplotlib.axis.XTick at 0x7f81ea7bb050>,\n",
              " <matplotlib.axis.XTick at 0x7f81ea70b490>,\n",
              " <matplotlib.axis.XTick at 0x7f81dda50590>,\n",
              " <matplotlib.axis.XTick at 0x7f825dfd4790>,\n",
              " <matplotlib.axis.XTick at 0x7f825e106d50>,\n",
              " <matplotlib.axis.XTick at 0x7f81e305be90>,\n",
              " <matplotlib.axis.XTick at 0x7f81df430490>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%'),\n",
              " <matplotlib.axis.XTick at 0x7f81dd83c2d0>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd83c390>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd81cbd0>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd65b4d0>,\n",
              " <matplotlib.axis.XTick at 0x7f81e3381a90>,\n",
              " <matplotlib.axis.XTick at 0x7f81e33816d0>,\n",
              " <matplotlib.axis.XTick at 0x7f81eaa692d0>,\n",
              " <matplotlib.axis.XTick at 0x7f81ea7bb390>,\n",
              " <matplotlib.axis.XTick at 0x7f81da3accd0>,\n",
              " <matplotlib.axis.XTick at 0x7f8260eca050>,\n",
              " <matplotlib.axis.XTick at 0x7f81ddccfad0>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%'),\n",
              " <matplotlib.axis.XTick at 0x7f81dd61bbd0>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd61bb90>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd61b910>,\n",
              " <matplotlib.axis.XTick at 0x7f8353c7a0d0>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd5f5250>,\n",
              " <matplotlib.axis.XTick at 0x7f81dea8a650>,\n",
              " <matplotlib.axis.XTick at 0x7f81dd83c650>,\n",
              " <matplotlib.axis.XTick at 0x7f8353c94c50>,\n",
              " <matplotlib.axis.XTick at 0x7f81da3acfd0>,\n",
              " <matplotlib.axis.XTick at 0x7f81e765b350>,\n",
              " <matplotlib.axis.XTick at 0x7f83541dd810>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%')]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack a perturbation trained model\n",
        "\n",
        "- We will apply augmentations to the dataset and re-train the target model.\n",
        "- The attacker **does not** know our augmentation settings, so he will train with a normal dataset of zero augmentations\n",
        "- We want to measure the quality of the attack when the target tries to defend MIAs by adding perturbed images of data samples "
      ],
      "metadata": {
        "id": "WZOpEdcmCMXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will defend against the same rotations and translations that the attack models uses (worst case for the attacker)\n",
        "rotates = create_rotates(r)\n",
        "translates = create_translates(d)\n",
        "\n",
        "X_train_aug = train_images\n",
        "X_eval_aug = eval_images\n",
        "y_train_aug = np.concatenate(tuple([train_labels] + [train_labels for rot in rotates] + [train_labels for tra in translates]))\n",
        "y_eval_aug = np.concatenate(tuple([eval_labels] + [eval_labels for rot in rotates] + [eval_labels for tra in translates]))\n",
        "\n",
        "\n",
        "\n",
        "for rot in rotates:\n",
        "  aug_x = apply_augment(train_images, rot, 'r')\n",
        "  X_train_aug = np.concatenate((X_train_aug,aug_x))\n",
        "  aug_x = apply_augment(eval_images, rot, 'r')\n",
        "  X_eval_aug = np.concatenate((X_eval_aug,aug_x))\n",
        "\n",
        "for tra in translates:\n",
        "  aug_x = apply_augment(train_images, tra, 'd')\n",
        "  X_train_aug = np.concatenate((X_train_aug,aug_x))\n",
        "  aug_x = apply_augment(eval_images, tra, 'd')\n",
        "  X_eval_aug = np.concatenate((X_eval_aug ,aug_x))\n"
      ],
      "metadata": {
        "id": "4Cl0sR-1I_sG"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  X_train_aug = tf.convert_to_tensor(X_train_aug)\n",
        "  y_train_aug = tf.convert_to_tensor(y_train_aug)\n",
        "  X_eval_aug = tf.convert_to_tensor(X_eval_aug)\n",
        "  y_eval_aug = tf.convert_to_tensor(y_eval_aug)\n",
        "  target_model = f_target(X_train_aug, y_train_aug, X_eval_aug, y_eval_aug, epochs=10) "
      ],
      "metadata": {
        "id": "JaNvyxztuwED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1023d70c-6b5e-4463-8e6c-1db9f5d98d48"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "688/688 [==============================] - 10s 14ms/step - loss: 1.1850 - accuracy: 0.6403 - val_loss: 3.0155 - val_accuracy: 0.3853\n",
            "Epoch 2/10\n",
            "688/688 [==============================] - 9s 13ms/step - loss: 0.1942 - accuracy: 0.9388 - val_loss: 3.5806 - val_accuracy: 0.3740\n",
            "Epoch 3/10\n",
            "688/688 [==============================] - 9s 13ms/step - loss: 0.1092 - accuracy: 0.9673 - val_loss: 4.3989 - val_accuracy: 0.3804\n",
            "Epoch 4/10\n",
            "688/688 [==============================] - 9s 13ms/step - loss: 0.0985 - accuracy: 0.9710 - val_loss: 4.5038 - val_accuracy: 0.4078\n",
            "Epoch 5/10\n",
            "688/688 [==============================] - 8s 12ms/step - loss: 0.0822 - accuracy: 0.9762 - val_loss: 5.0450 - val_accuracy: 0.3900\n",
            "Epoch 6/10\n",
            "688/688 [==============================] - 9s 13ms/step - loss: 0.0758 - accuracy: 0.9785 - val_loss: 5.1546 - val_accuracy: 0.3704\n",
            "Epoch 7/10\n",
            "688/688 [==============================] - 8s 12ms/step - loss: 0.0684 - accuracy: 0.9805 - val_loss: 6.0791 - val_accuracy: 0.3742\n",
            "Epoch 8/10\n",
            "688/688 [==============================] - 8s 12ms/step - loss: 0.0693 - accuracy: 0.9824 - val_loss: 5.9424 - val_accuracy: 0.3529\n",
            "Epoch 9/10\n",
            "688/688 [==============================] - 8s 12ms/step - loss: 0.0568 - accuracy: 0.9840 - val_loss: 6.0120 - val_accuracy: 0.3525\n",
            "Epoch 10/10\n",
            "688/688 [==============================] - 8s 12ms/step - loss: 0.0680 - accuracy: 0.9819 - val_loss: 7.3687 - val_accuracy: 0.3271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is quite overfitted so now all that is left is to evaluate the attack model we created before on the newly trained and \"defended\" target model with perturbations in the train dataset."
      ],
      "metadata": {
        "id": "bsRYuPrCUBWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_in = prepare_batch(target_model, train_images[:1000], train_labels[:1000], True)\n",
        "print(\"Testing with 'in' data only:\")\n",
        "res_in = evaluate_attack(attack_model_bundle, D_in[:, :-1], D_in[:, -1], 10)\n",
        "\n",
        "D_out = prepare_batch(target_model, attacker_images[:1000], attacker_labels[:1000], False)\n",
        "print(\"\\nTesting with 'out' data only:\")\n",
        "res_out = evaluate_attack(attack_model_bundle, D_out[:, :-1], D_out[:, -1], 10)\n",
        "\n",
        "print(\"\\nTesting with all prev data: \")\n",
        "res_all = evaluate_attack(attack_model_bundle, np.concatenate((D_out[:, :-1], D_in[:, :-1])), np.concatenate((D_out[:, -1], D_in[:, -1])), 10)\n",
        "\n",
        "print(f\"\\nTotal attack accuracy: {np.mean(res_all)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyt1sD06SBFa",
        "outputId": "a855f51a-95aa-49ce-ceb6-103057a59419"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with 'in' data only:\n",
            "class-1: 0.978723406791687\n",
            "class-2: 0.9807692170143127\n",
            "class-3: 0.9797979593276978\n",
            "class-4: 0.969072163105011\n",
            "class-5: 1.0\n",
            "class-6: 0.9894737005233765\n",
            "class-7: 0.9677419066429138\n",
            "class-8: 0.9900000095367432\n",
            "class-9: 0.8585858345031738\n",
            "class-10: 1.0\n",
            "\n",
            "Testing with 'out' data only:\n",
            "class-1: 0.6842105388641357\n",
            "class-2: 0.488095223903656\n",
            "class-3: 0.7804877758026123\n",
            "class-4: 0.8064516186714172\n",
            "class-5: 0.5416666865348816\n",
            "class-6: 0.6486486196517944\n",
            "class-7: 0.5961538553237915\n",
            "class-8: 0.6136363744735718\n",
            "class-9: 0.7452830076217651\n",
            "class-10: 0.7799999713897705\n",
            "\n",
            "Testing with all prev data: \n",
            "class-1: 0.8306878209114075\n",
            "class-2: 0.7606382966041565\n",
            "class-3: 0.869369387626648\n",
            "class-4: 0.8894736766815186\n",
            "class-5: 0.7962962985038757\n",
            "class-6: 0.8058252334594727\n",
            "class-7: 0.7715736031532288\n",
            "class-8: 0.813829779624939\n",
            "class-9: 0.800000011920929\n",
            "class-10: 0.8894472122192383\n",
            "\n",
            "Total attack accuracy: 0.8227141320705413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "To conclude if the model is more vulnerable, we must check what are the label divergence percentage in the newly trained model."
      ],
      "metadata": {
        "id": "fW_JINmBUgd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test it onthe same data as we tested the non-adjusted to augmentation model\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# tests in D_in\n",
        "diffs_per_class_D_in = study_perturbations(target_model, train_images[train_idx], train_labels[train_idx], r, d) \n",
        "total_diffs_in = sum(diffs_per_class_D_in)/10\n",
        "axes[0].set_title('D_in predicted label divergence');\n",
        "axes[0].barh(list(range(10)), diffs_per_class_D_in, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "# tests in D_out  \n",
        "diffs_per_class_D_out= study_perturbations(target_model,attacker_images[test_idx],attacker_labels[test_idx], r, d)\n",
        "total_diffs_out = sum(diffs_per_class_D_out)/10\n",
        "axes[1].set_title('D_out predicted label divergence');\n",
        "axes[1].barh(list(range(10)), diffs_per_class_D_out, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "axes[2].set_title('Total predicted label divergence percentage');\n",
        "axes[2].barh([1, 0], [total_diffs_in, total_diffs_out], tick_label=['In', 'Out'])\n",
        "\n",
        "plt.setp(axes, xticks=range(0, 101, 10), xticklabels=[f'{i}%' for i in range(0, 101, 10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OWDU6N1uSCnA",
        "outputId": "efe35e72-6794-46c5-8043-4f242b271384"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.axis.XTick at 0x7f81de618410>,\n",
              " <matplotlib.axis.XTick at 0x7f81e3426e50>,\n",
              " <matplotlib.axis.XTick at 0x7f81e3426ed0>,\n",
              " <matplotlib.axis.XTick at 0x7f825e350f10>,\n",
              " <matplotlib.axis.XTick at 0x7f81de92bb50>,\n",
              " <matplotlib.axis.XTick at 0x7f825e05e710>,\n",
              " <matplotlib.axis.XTick at 0x7f825e22db90>,\n",
              " <matplotlib.axis.XTick at 0x7f825de01610>,\n",
              " <matplotlib.axis.XTick at 0x7f81ea693590>,\n",
              " <matplotlib.axis.XTick at 0x7f82609d8590>,\n",
              " <matplotlib.axis.XTick at 0x7f825e05eb50>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%'),\n",
              " <matplotlib.axis.XTick at 0x7f81ea7fc590>,\n",
              " <matplotlib.axis.XTick at 0x7f81ea7fccd0>,\n",
              " <matplotlib.axis.XTick at 0x7f825e08f450>,\n",
              " <matplotlib.axis.XTick at 0x7f825e08fa90>,\n",
              " <matplotlib.axis.XTick at 0x7f82608d4b90>,\n",
              " <matplotlib.axis.XTick at 0x7f825e351950>,\n",
              " <matplotlib.axis.XTick at 0x7f81dbd8e190>,\n",
              " <matplotlib.axis.XTick at 0x7f81dbd8e410>,\n",
              " <matplotlib.axis.XTick at 0x7f82605f7e50>,\n",
              " <matplotlib.axis.XTick at 0x7f825e351cd0>,\n",
              " <matplotlib.axis.XTick at 0x7f825e08fc90>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%'),\n",
              " <matplotlib.axis.XTick at 0x7f825e2cd510>,\n",
              " <matplotlib.axis.XTick at 0x7f8260c97f90>,\n",
              " <matplotlib.axis.XTick at 0x7f82605f7b10>,\n",
              " <matplotlib.axis.XTick at 0x7f81de744c10>,\n",
              " <matplotlib.axis.XTick at 0x7f81de744fd0>,\n",
              " <matplotlib.axis.XTick at 0x7f81eaa18dd0>,\n",
              " <matplotlib.axis.XTick at 0x7f825e363690>,\n",
              " <matplotlib.axis.XTick at 0x7f825e3632d0>,\n",
              " <matplotlib.axis.XTick at 0x7f825e3639d0>,\n",
              " <matplotlib.axis.XTick at 0x7f8260c5ac90>,\n",
              " <matplotlib.axis.XTick at 0x7f825e363950>,\n",
              " Text(0, 0, '0%'),\n",
              " Text(0, 0, '10%'),\n",
              " Text(0, 0, '20%'),\n",
              " Text(0, 0, '30%'),\n",
              " Text(0, 0, '40%'),\n",
              " Text(0, 0, '50%'),\n",
              " Text(0, 0, '60%'),\n",
              " Text(0, 0, '70%'),\n",
              " Text(0, 0, '80%'),\n",
              " Text(0, 0, '90%'),\n",
              " Text(0, 0, '100%')]"
            ]
          },
          "metadata": {},
          "execution_count": 181
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the general percentage of predicted label divergence has fallen, **but** the confidence of the ML algorithm in predicting the label of perturbed instances of instances in $D_{in}$ is even higher that before. This means that the adjusted model is even more vulnerable. Next step is to run all that with a well-generalized model and tune the attack model to get max accuracy."
      ],
      "metadata": {
        "id": "VVyVo6yukPGH"
      }
    }
  ]
}