{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cifar_10_mia_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6M8ieHE-a_s"
      },
      "source": [
        "# Membership Inference Attack Revisited\n",
        "\n",
        "Attack Model:\n",
        "- Fairly accurate target model $f_{target}(x)$, CIFAR-10 classification model\n",
        "- Attacker knows nothing about the architecture of the target model and creates his own NN architecture\n",
        "- Attacker has no information in the dataset (TODO). Attacker uses query based datapoint generation (check Shadow Datasets generation section)\n",
        "- $D_{target}$ training dataset and $\\cup_i D_{shadow_i}$ dataset are disjoint\n",
        "\n",
        "\n",
        "The target is to prove that in a **complete black-box scenario** the attacker can exploit every minor model leak.\n",
        "\n",
        "CHANGES from previous session: \n",
        "- Target model is more generalized\n",
        "- The attack model doesn't use c-different classifier, but 1 sole classifier and the learning insances contain the datapoint's class as a feature\n",
        "- (TODO) The attack model only return a label as prediction and not a prediction vector\n",
        "- (TODO) Use data synthesis algorithm\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg_WQJ7j5n3B",
        "outputId": "7c59ebfc-fa86-4ed5-a003-e2f34146d4b4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kv7Qon0qRTk"
      },
      "source": [
        "## Basic Models\n",
        "\n",
        "Create basic model functions\n",
        "- __Target model__: A small and simple CNN\n",
        "- __N Shadow Models__: Same architecture as the target model\n",
        "- __Attack Models__ : One for every class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qg7LkXX0zOF"
      },
      "source": [
        "def f_target(X_train, y_train, X_test=None, y_test=None, epochs=10):\n",
        "  \"\"\"\n",
        "  Returns a trained target model, if test data are specified we will evaluate the model and print its accuracy\n",
        "  \"\"\"\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(10))\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  if X_test is None or y_test is None:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_split=0.2)\n",
        "  else:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test), verbose=True)\n",
        "  return model"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7QZHFWE8exF"
      },
      "source": [
        "def f_shadow(X_train, y_train, X_test=None, y_test=None, epochs=10):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='tanh', input_shape=(32, 32, 3)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='tanh'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='tanh'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(128, activation='tanh'))\n",
        "  model.add(layers.Dense(10)   )\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  if X_test is None or y_test is None:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_split=0.2)\n",
        "  else:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test))\n",
        "  return model"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co_IdUV58hgK"
      },
      "source": [
        "def __f_attack(X_train, y_train, X_test, y_test, epochs=30):\n",
        "  print(X_train.shape, X_test.shape)\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(X_train.shape[1], )))\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(1000, activation='relu'))\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "  model.add(layers.Dense(2))\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test), verbose=True)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def f_attack(X, y):\n",
        "  # X_i = (class, probability vector, )\n",
        "  classes = np.unique(train_labels) # all class labels\n",
        "  with tf.device('/gpu:0'):\n",
        "  # split to train and test datasets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)\n",
        "    attack_model = __f_attack(X_train, y_train, X_test, y_test) \n",
        "  return attack_model"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy2NLipP75sX"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "  train_images = train_images[:20000] # as the paper attack train wiht only 200 records\n",
        "  train_labels = train_labels[:20000]\n",
        "  # use the rest as testing - 'out' records\n",
        "  test_images = np.concatenate((train_images[20000:], test_images))\n",
        "  test_labels = np.concatenate((train_labels[20000:], test_labels))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap02yKRT76RJ",
        "outputId": "88035054-3f08-4984-d9ac-098e2221e5cb"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  target_model = f_target(train_images, train_labels) "
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.2996 - accuracy: 0.2545 - val_loss: 1.6339 - val_accuracy: 0.4070\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.4862 - accuracy: 0.4660 - val_loss: 1.4012 - val_accuracy: 0.5045\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.2742 - accuracy: 0.5436 - val_loss: 1.3131 - val_accuracy: 0.5378\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.1299 - accuracy: 0.5991 - val_loss: 1.2537 - val_accuracy: 0.5642\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.0123 - accuracy: 0.6362 - val_loss: 1.2689 - val_accuracy: 0.5715\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9086 - accuracy: 0.6806 - val_loss: 1.2796 - val_accuracy: 0.5692\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8062 - accuracy: 0.7193 - val_loss: 1.3913 - val_accuracy: 0.5767\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.7172 - accuracy: 0.7484 - val_loss: 1.4171 - val_accuracy: 0.5773\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.6376 - accuracy: 0.7778 - val_loss: 1.5729 - val_accuracy: 0.5660\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.5996 - accuracy: 0.7906 - val_loss: 1.5951 - val_accuracy: 0.5805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkIHiM-a78wS"
      },
      "source": [
        "# return prediction vector\n",
        "def predict(model, X_i):\n",
        "  prob_layer = layers.Softmax()\n",
        "  return prob_layer(model(X_i.reshape((1, 32, 32, 3)))).numpy()[0]\n",
        "\n",
        "def rand_record(X=None, k=1):\n",
        "  if X is None:\n",
        "    # create a whole new record\n",
        "    X = np.random.randint(0, 255+1, size=32*32*3).reshape((32, 32, 3))\n",
        "  else:\n",
        "    X = X.reshape((32*32*3))\n",
        "    # change k random features\n",
        "    k_features = np.random.choice(range(X.shape[0]), size=k, replace=False)\n",
        "\n",
        "    for i in k_features:\n",
        "      X[i] += np.random.randint(-X[i], 256-X[i]) # subtract/add a number to change the feature \n",
        "  \n",
        "  return X.reshape((32,32,3))\n",
        "\n",
        "def synthesize(c, target_model, k_min, k_max, conf_min, iter_max, rej_max):\n",
        "  X = rand_record()\n",
        "  y_conf_star = 0.0\n",
        "  j = 0\n",
        "  k = k_max\n",
        "  X_star = None\n",
        "\n",
        "  for iter in range(iter_max):\n",
        "    y = predict(target_model, X);\n",
        "    if y[c] >= y_conf_star:\n",
        "      if y[c] > conf_min and c == np.argmax(y):\n",
        "        # sample to decide if we return the data\n",
        "        if np.random.randint(0, 2):\n",
        "          return X\n",
        "      y_conf_star = y[c]\n",
        "      j = 0\n",
        "      X_star = X\n",
        "    else:\n",
        "      # reject and resample X\n",
        "      j += 1\n",
        "\n",
        "      if j > rej_max:\n",
        "        k = max(k_min, math.ceil(k/2))\n",
        "        j = 0\n",
        "    \n",
        "    X = rand_record(X_star, k)\n",
        "  \n",
        "  return X_star # failed. return the last successfull record"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sliyw7xO2PTg"
      },
      "source": [
        "\n",
        "def divide_dataset(n_shadows, shadow_dataset_size, X, y):\n",
        "  D_shadows = []\n",
        "  rng = np.random.default_rng()\n",
        "  for i in range(n_shadows):\n",
        "    sample_i = np.random.choice(range(X.shape[0]), shadow_dataset_size, replace=False)\n",
        "    assert np.unique(sample_i).shape[0] == shadow_dataset_size # sanity check\n",
        "    D_shadows.append((X[sample_i, :], y[sample_i, :]))\n",
        "  return D_shadows\n",
        "\n",
        "# returns a list of 'n_shadows' datasets\n",
        "def generate_shadow_dataset(target_model, n_shadows, shadow_dataset_size, n_classes, X_test=None, y_test=None):\n",
        "  # in case we give test data we will just divide those to train the shadow models\n",
        "  if X_test is not None and y_test is not None:\n",
        "    return divide_dataset(n_shadows, shadow_dataset_size, X_test, y_test)\n",
        "  \n",
        "  \n",
        "  # helper function to return a datapoint (for sure)\n",
        "  def get_shadow_datapoint(c):\n",
        "    X_i = None\n",
        "    with tf.device('/gpu:0'):\n",
        "      while X_i is None:\n",
        "        X_i = synthesize(c, target_model, 1, 32*32*3, 0.65, 100, 5)\n",
        "    return X_i\n",
        "  \n",
        "  D_shadows = []\n",
        "  for i in range(n_shadows):\n",
        "    print(f\"Generating D_shadow_{i}\")\n",
        "    # uniformly generate X's for all of the classes (y's)\n",
        "    X_shadow = np.asarray(\n",
        "        [get_shadow_datapoint(i%n_classes) for i in range(shadow_dataset_size)]\n",
        "    )\n",
        "    y_shadow = np.asarray(\n",
        "        [(i%n_classes) for i in range(shadow_dataset_size)]\n",
        "    ).reshape((-1, 1))\n",
        "\n",
        "    D_shadows.append((X_shadow, y_shadow))\n",
        "  \n",
        "  return D_shadows\n",
        "\n",
        "def create_shadows(D_shadows):\n",
        "  shadow_models = [] # shadow model list\n",
        "\n",
        "  for D_shadow in D_shadows:\n",
        "    # sample data to feed/evaluate the model\n",
        "    X_shadow, y_shadow = D_shadow\n",
        "    shadow_X_train, shadow_X_test, shadow_y_train, shadow_y_test = train_test_split(X_shadow, y_shadow, shuffle=True, test_size=0.2)\n",
        "\n",
        "    # generate the shadow model\n",
        "    shadow_model = f_shadow(shadow_X_train, shadow_y_train, shadow_X_test, shadow_y_test)\n",
        "\n",
        "    D_shadow = (shadow_X_train, shadow_y_train), (shadow_X_test, shadow_y_test)\n",
        "    shadow_models.append((shadow_model, D_shadow))\n",
        "\n",
        "  return shadow_models # return a list where every item is (model, acc), train-data, test-data"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ziH9LP3CY5"
      },
      "source": [
        "# generate shadow datasets\n",
        "D_shadows = generate_shadow_dataset(target_model, 15, 5000, 10, test_images, test_labels)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dc8morl-NRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7eacc4-9e3c-4991-f672-0439a6923566"
      },
      "source": [
        "# train the shadow models\n",
        "shadow_models = create_shadows(D_shadows)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.9167 - accuracy: 0.3305 - val_loss: 1.7599 - val_accuracy: 0.3700\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5741 - accuracy: 0.4448 - val_loss: 1.5704 - val_accuracy: 0.4310\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3856 - accuracy: 0.5175 - val_loss: 1.5403 - val_accuracy: 0.4610\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1757 - accuracy: 0.5922 - val_loss: 1.5040 - val_accuracy: 0.4910\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.0116 - accuracy: 0.6428 - val_loss: 1.5046 - val_accuracy: 0.4830\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.7979 - accuracy: 0.7380 - val_loss: 1.4235 - val_accuracy: 0.5130\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.5833 - accuracy: 0.8260 - val_loss: 1.5024 - val_accuracy: 0.5190\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4166 - accuracy: 0.8990 - val_loss: 1.5000 - val_accuracy: 0.5150\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2937 - accuracy: 0.9340 - val_loss: 1.5389 - val_accuracy: 0.5090\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1742 - accuracy: 0.9778 - val_loss: 1.6067 - val_accuracy: 0.5090\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.8917 - accuracy: 0.3315 - val_loss: 1.7214 - val_accuracy: 0.3840\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5423 - accuracy: 0.4515 - val_loss: 1.5509 - val_accuracy: 0.4390\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.3519 - accuracy: 0.5210 - val_loss: 1.6076 - val_accuracy: 0.4380\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.1427 - accuracy: 0.5995 - val_loss: 1.5453 - val_accuracy: 0.4460\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9519 - accuracy: 0.6712 - val_loss: 1.6506 - val_accuracy: 0.4630\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.7871 - accuracy: 0.7398 - val_loss: 1.4944 - val_accuracy: 0.4890\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.6178 - accuracy: 0.8060 - val_loss: 1.5158 - val_accuracy: 0.4970\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4493 - accuracy: 0.8730 - val_loss: 1.4621 - val_accuracy: 0.5240\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2829 - accuracy: 0.9383 - val_loss: 1.5558 - val_accuracy: 0.5250\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1617 - accuracy: 0.9787 - val_loss: 1.5428 - val_accuracy: 0.5230\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.8799 - accuracy: 0.3380 - val_loss: 1.7220 - val_accuracy: 0.3550\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5093 - accuracy: 0.4563 - val_loss: 1.5045 - val_accuracy: 0.4630\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3106 - accuracy: 0.5393 - val_loss: 1.4625 - val_accuracy: 0.4730\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.0801 - accuracy: 0.6260 - val_loss: 1.4382 - val_accuracy: 0.4910\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.8916 - accuracy: 0.6977 - val_loss: 1.3975 - val_accuracy: 0.5290\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.6746 - accuracy: 0.7847 - val_loss: 1.4154 - val_accuracy: 0.5300\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4937 - accuracy: 0.8637 - val_loss: 1.4837 - val_accuracy: 0.5190\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3272 - accuracy: 0.9300 - val_loss: 1.4794 - val_accuracy: 0.5140\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2019 - accuracy: 0.9712 - val_loss: 1.5282 - val_accuracy: 0.5180\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1258 - accuracy: 0.9898 - val_loss: 1.5477 - val_accuracy: 0.5280\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 2.0018 - accuracy: 0.3020 - val_loss: 1.6979 - val_accuracy: 0.3730\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.6212 - accuracy: 0.4165 - val_loss: 1.5805 - val_accuracy: 0.4460\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.4082 - accuracy: 0.4960 - val_loss: 1.5064 - val_accuracy: 0.4600\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.2210 - accuracy: 0.5725 - val_loss: 1.4455 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9950 - accuracy: 0.6557 - val_loss: 1.4204 - val_accuracy: 0.5080\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.8091 - accuracy: 0.7325 - val_loss: 1.4290 - val_accuracy: 0.4880\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.6134 - accuracy: 0.8135 - val_loss: 1.4545 - val_accuracy: 0.5110\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4359 - accuracy: 0.8873 - val_loss: 1.4671 - val_accuracy: 0.5220\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2938 - accuracy: 0.9383 - val_loss: 1.5119 - val_accuracy: 0.5200\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1889 - accuracy: 0.9735 - val_loss: 1.4997 - val_accuracy: 0.5290\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.9162 - accuracy: 0.3205 - val_loss: 1.6602 - val_accuracy: 0.3880\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5863 - accuracy: 0.4355 - val_loss: 1.5990 - val_accuracy: 0.4230\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.4089 - accuracy: 0.4995 - val_loss: 1.5876 - val_accuracy: 0.4400\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.1752 - accuracy: 0.5885 - val_loss: 1.4934 - val_accuracy: 0.4570\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9715 - accuracy: 0.6690 - val_loss: 1.4649 - val_accuracy: 0.4810\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7681 - accuracy: 0.7542 - val_loss: 1.5092 - val_accuracy: 0.4740\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5806 - accuracy: 0.8307 - val_loss: 1.4602 - val_accuracy: 0.5140\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4211 - accuracy: 0.8913 - val_loss: 1.4638 - val_accuracy: 0.5040\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2635 - accuracy: 0.9507 - val_loss: 1.5280 - val_accuracy: 0.5060\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1626 - accuracy: 0.9797 - val_loss: 1.5730 - val_accuracy: 0.5190\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9145 - accuracy: 0.3075 - val_loss: 1.6504 - val_accuracy: 0.3740\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5985 - accuracy: 0.4263 - val_loss: 1.5536 - val_accuracy: 0.4380\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.4073 - accuracy: 0.4942 - val_loss: 1.4735 - val_accuracy: 0.4790\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1759 - accuracy: 0.5810 - val_loss: 1.4338 - val_accuracy: 0.5020\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.0093 - accuracy: 0.6497 - val_loss: 1.4275 - val_accuracy: 0.5120\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.8003 - accuracy: 0.7362 - val_loss: 1.3400 - val_accuracy: 0.5450\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.6162 - accuracy: 0.8080 - val_loss: 1.4215 - val_accuracy: 0.5270\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4639 - accuracy: 0.8792 - val_loss: 1.4308 - val_accuracy: 0.5080\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2817 - accuracy: 0.9448 - val_loss: 1.4394 - val_accuracy: 0.5470\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1727 - accuracy: 0.9790 - val_loss: 1.4521 - val_accuracy: 0.5410\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.9058 - accuracy: 0.3270 - val_loss: 1.6525 - val_accuracy: 0.3970\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.4907 - accuracy: 0.4555 - val_loss: 1.4893 - val_accuracy: 0.4540\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.2610 - accuracy: 0.5595 - val_loss: 1.4120 - val_accuracy: 0.4830\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.0780 - accuracy: 0.6313 - val_loss: 1.3997 - val_accuracy: 0.4950\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.9005 - accuracy: 0.6960 - val_loss: 1.3574 - val_accuracy: 0.5260\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.6315 - accuracy: 0.8020 - val_loss: 1.3989 - val_accuracy: 0.4930\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4981 - accuracy: 0.8605 - val_loss: 1.4947 - val_accuracy: 0.5190\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3431 - accuracy: 0.9162 - val_loss: 1.4003 - val_accuracy: 0.5350\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2169 - accuracy: 0.9632 - val_loss: 1.4449 - val_accuracy: 0.5380\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1127 - accuracy: 0.9912 - val_loss: 1.4908 - val_accuracy: 0.5300\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.8934 - accuracy: 0.3332 - val_loss: 1.7768 - val_accuracy: 0.3360\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5543 - accuracy: 0.4460 - val_loss: 1.6721 - val_accuracy: 0.4010\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.3436 - accuracy: 0.5282 - val_loss: 1.5464 - val_accuracy: 0.4560\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1561 - accuracy: 0.5997 - val_loss: 1.5131 - val_accuracy: 0.4690\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.9588 - accuracy: 0.6650 - val_loss: 1.5053 - val_accuracy: 0.4680\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.7625 - accuracy: 0.7455 - val_loss: 1.4873 - val_accuracy: 0.5170\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5685 - accuracy: 0.8278 - val_loss: 1.5005 - val_accuracy: 0.5020\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3838 - accuracy: 0.9068 - val_loss: 1.4477 - val_accuracy: 0.5210\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2301 - accuracy: 0.9628 - val_loss: 1.4765 - val_accuracy: 0.5360\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1415 - accuracy: 0.9868 - val_loss: 1.5508 - val_accuracy: 0.5340\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 0.3340 - val_loss: 1.7344 - val_accuracy: 0.3730\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5059 - accuracy: 0.4568 - val_loss: 1.7809 - val_accuracy: 0.3790\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.3682 - accuracy: 0.5178 - val_loss: 1.5321 - val_accuracy: 0.4640\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1758 - accuracy: 0.5807 - val_loss: 1.4870 - val_accuracy: 0.4670\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.0091 - accuracy: 0.6507 - val_loss: 1.4913 - val_accuracy: 0.4810\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.8027 - accuracy: 0.7390 - val_loss: 1.5200 - val_accuracy: 0.4750\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.6212 - accuracy: 0.8055 - val_loss: 1.5612 - val_accuracy: 0.5080\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4489 - accuracy: 0.8838 - val_loss: 1.5628 - val_accuracy: 0.5030\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3061 - accuracy: 0.9308 - val_loss: 1.7231 - val_accuracy: 0.4980\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2008 - accuracy: 0.9680 - val_loss: 1.6251 - val_accuracy: 0.4880\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.9401 - accuracy: 0.3045 - val_loss: 1.7254 - val_accuracy: 0.3590\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.6232 - accuracy: 0.4042 - val_loss: 1.6723 - val_accuracy: 0.3930\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.4843 - accuracy: 0.4620 - val_loss: 1.5037 - val_accuracy: 0.4770\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.2832 - accuracy: 0.5490 - val_loss: 1.4890 - val_accuracy: 0.4610\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1114 - accuracy: 0.6112 - val_loss: 1.4878 - val_accuracy: 0.4790\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9262 - accuracy: 0.6787 - val_loss: 1.4266 - val_accuracy: 0.5290\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7382 - accuracy: 0.7650 - val_loss: 1.4925 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5503 - accuracy: 0.8418 - val_loss: 1.4344 - val_accuracy: 0.5380\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3917 - accuracy: 0.9080 - val_loss: 1.4627 - val_accuracy: 0.5340\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2599 - accuracy: 0.9528 - val_loss: 1.4996 - val_accuracy: 0.5520\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9313 - accuracy: 0.3205 - val_loss: 1.6606 - val_accuracy: 0.3960\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5501 - accuracy: 0.4415 - val_loss: 1.6468 - val_accuracy: 0.4260\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3612 - accuracy: 0.5215 - val_loss: 1.4741 - val_accuracy: 0.4710\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1221 - accuracy: 0.6055 - val_loss: 1.5051 - val_accuracy: 0.4580\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9454 - accuracy: 0.6780 - val_loss: 1.4547 - val_accuracy: 0.4860\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7587 - accuracy: 0.7540 - val_loss: 1.4308 - val_accuracy: 0.5060\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.6082 - accuracy: 0.8023 - val_loss: 1.4940 - val_accuracy: 0.4970\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4052 - accuracy: 0.8935 - val_loss: 1.5177 - val_accuracy: 0.5160\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2752 - accuracy: 0.9370 - val_loss: 1.6589 - val_accuracy: 0.5080\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1780 - accuracy: 0.9735 - val_loss: 1.6043 - val_accuracy: 0.5120\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.0154 - accuracy: 0.2860 - val_loss: 1.7953 - val_accuracy: 0.3360\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.6883 - accuracy: 0.3840 - val_loss: 1.8237 - val_accuracy: 0.3730\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5274 - accuracy: 0.4435 - val_loss: 1.7211 - val_accuracy: 0.3820\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3929 - accuracy: 0.5052 - val_loss: 1.6291 - val_accuracy: 0.4180\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1769 - accuracy: 0.5770 - val_loss: 1.5735 - val_accuracy: 0.4370\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.0045 - accuracy: 0.6500 - val_loss: 1.7284 - val_accuracy: 0.3800\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.8199 - accuracy: 0.7343 - val_loss: 1.6190 - val_accuracy: 0.4460\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.6398 - accuracy: 0.8083 - val_loss: 1.5820 - val_accuracy: 0.4770\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4551 - accuracy: 0.8820 - val_loss: 1.6122 - val_accuracy: 0.4770\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3123 - accuracy: 0.9355 - val_loss: 1.6707 - val_accuracy: 0.4550\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9316 - accuracy: 0.3207 - val_loss: 1.7735 - val_accuracy: 0.3770\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5593 - accuracy: 0.4367 - val_loss: 1.6417 - val_accuracy: 0.4050\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3762 - accuracy: 0.5090 - val_loss: 1.5698 - val_accuracy: 0.4100\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1745 - accuracy: 0.5740 - val_loss: 1.5627 - val_accuracy: 0.4430\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9616 - accuracy: 0.6715 - val_loss: 1.5404 - val_accuracy: 0.4500\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7809 - accuracy: 0.7410 - val_loss: 1.6205 - val_accuracy: 0.4570\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5929 - accuracy: 0.8227 - val_loss: 1.5220 - val_accuracy: 0.5030\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.4152 - accuracy: 0.8947 - val_loss: 1.5205 - val_accuracy: 0.4980\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2825 - accuracy: 0.9435 - val_loss: 1.5295 - val_accuracy: 0.5130\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1800 - accuracy: 0.9743 - val_loss: 1.6225 - val_accuracy: 0.4990\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 11ms/step - loss: 1.8825 - accuracy: 0.3392 - val_loss: 1.6473 - val_accuracy: 0.4020\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5034 - accuracy: 0.4675 - val_loss: 1.7981 - val_accuracy: 0.3750\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3109 - accuracy: 0.5300 - val_loss: 1.4776 - val_accuracy: 0.4780\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1102 - accuracy: 0.6095 - val_loss: 1.5201 - val_accuracy: 0.4650\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9342 - accuracy: 0.6825 - val_loss: 1.5285 - val_accuracy: 0.4780\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7474 - accuracy: 0.7530 - val_loss: 1.6038 - val_accuracy: 0.4710\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5330 - accuracy: 0.8450 - val_loss: 1.5264 - val_accuracy: 0.4870\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3666 - accuracy: 0.9100 - val_loss: 1.5776 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2293 - accuracy: 0.9610 - val_loss: 1.5771 - val_accuracy: 0.5180\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.1437 - accuracy: 0.9818 - val_loss: 1.6389 - val_accuracy: 0.5200\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.0274 - accuracy: 0.2718 - val_loss: 1.8684 - val_accuracy: 0.3430\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.6601 - accuracy: 0.3927 - val_loss: 1.7068 - val_accuracy: 0.3790\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5055 - accuracy: 0.4577 - val_loss: 1.5619 - val_accuracy: 0.4530\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.3458 - accuracy: 0.5263 - val_loss: 1.5547 - val_accuracy: 0.4300\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.1370 - accuracy: 0.6083 - val_loss: 1.5109 - val_accuracy: 0.4450\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.9612 - accuracy: 0.6708 - val_loss: 1.4658 - val_accuracy: 0.4970\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7475 - accuracy: 0.7645 - val_loss: 1.4990 - val_accuracy: 0.4840\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.5426 - accuracy: 0.8472 - val_loss: 1.4716 - val_accuracy: 0.4820\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.3978 - accuracy: 0.9078 - val_loss: 1.5426 - val_accuracy: 0.4970\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2529 - accuracy: 0.9575 - val_loss: 1.5198 - val_accuracy: 0.5080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6lVDDuD2DA5"
      },
      "source": [
        "# helper function to prepare each shadow dataset batch\n",
        "def prepare_batch(shadow_model, X, y, in_D=True):\n",
        "  #decide membership\n",
        "  y_member = np.ones(shape=(y.shape[0], 1)) if in_D else np.zeros(shape=(y.shape[0], 1))\n",
        "  \n",
        "  # get probability vector\n",
        "  prob_layer = layers.Softmax() # probability layer implementing softmax for mapping NN results to probabilities in [0, 1]\n",
        "  prob_vec = prob_layer(shadow_model(X)).numpy()\n",
        "  \n",
        "  # return an instance <actual class, prob_vec from shadow model, 'in'/'out' D_target membership> \n",
        "  return np.concatenate((y.reshape(-1, 1), prob_vec, y_member), axis=1)\n",
        "\n",
        "def generate_attack_dataset(shadow_models):\n",
        "  # input is a list where items are model, (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "  D_attack = None\n",
        "  # D_attack_i format = <class, prob_vec, membership label (1 or 0)> \n",
        "  for shadow_model, ((X_train, y_train), (X_test, y_test)) in shadow_models:\n",
        "    batch = np.concatenate((\n",
        "        prepare_batch(shadow_model, X_train, y_train, True), # members of shadow dataset \n",
        "        prepare_batch(shadow_model, X_test, y_test, False)   # non members of shadow dataset\n",
        "    ))   \n",
        "\n",
        "    D_attack = np.concatenate((D_attack, batch)) if D_attack is not None else batch  \n",
        "\n",
        "  return D_attack "
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_yfm1hK-Sg7"
      },
      "source": [
        "D_attack = generate_attack_dataset(shadow_models)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhE29HBBYGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ba05a0-8b7f-4678-a27e-8703c6e66175"
      },
      "source": [
        "attack_model = f_attack(D_attack[:, :-1], D_attack[:, -1])"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 11) (15000, 11)\n",
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4466 - accuracy: 0.8334 - val_loss: 0.4299 - val_accuracy: 0.8424\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3910 - accuracy: 0.8601 - val_loss: 0.3284 - val_accuracy: 0.8885\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3330 - accuracy: 0.8819 - val_loss: 0.3176 - val_accuracy: 0.8885\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3211 - accuracy: 0.8874 - val_loss: 0.3140 - val_accuracy: 0.8885\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3161 - accuracy: 0.8874 - val_loss: 0.3226 - val_accuracy: 0.8843\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3124 - accuracy: 0.8902 - val_loss: 0.3068 - val_accuracy: 0.8912\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3093 - accuracy: 0.8909 - val_loss: 0.3017 - val_accuracy: 0.8941\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3077 - accuracy: 0.8916 - val_loss: 0.2975 - val_accuracy: 0.8949\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3063 - accuracy: 0.8921 - val_loss: 0.2992 - val_accuracy: 0.8953\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3051 - accuracy: 0.8928 - val_loss: 0.2983 - val_accuracy: 0.8945\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3041 - accuracy: 0.8932 - val_loss: 0.2983 - val_accuracy: 0.8943\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3038 - accuracy: 0.8935 - val_loss: 0.2984 - val_accuracy: 0.8948\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3038 - accuracy: 0.8928 - val_loss: 0.2952 - val_accuracy: 0.8951\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3022 - accuracy: 0.8936 - val_loss: 0.2953 - val_accuracy: 0.8955\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3018 - accuracy: 0.8934 - val_loss: 0.2933 - val_accuracy: 0.8987\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3008 - accuracy: 0.8948 - val_loss: 0.2922 - val_accuracy: 0.8970\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3006 - accuracy: 0.8949 - val_loss: 0.2910 - val_accuracy: 0.8967\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2996 - accuracy: 0.8949 - val_loss: 0.2932 - val_accuracy: 0.8973\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2999 - accuracy: 0.8947 - val_loss: 0.2948 - val_accuracy: 0.8967\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2991 - accuracy: 0.8949 - val_loss: 0.2920 - val_accuracy: 0.8959\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2992 - accuracy: 0.8948 - val_loss: 0.2933 - val_accuracy: 0.8972\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2991 - accuracy: 0.8949 - val_loss: 0.2898 - val_accuracy: 0.8977\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2983 - accuracy: 0.8947 - val_loss: 0.2909 - val_accuracy: 0.8977\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2981 - accuracy: 0.8955 - val_loss: 0.2970 - val_accuracy: 0.8945\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2975 - accuracy: 0.8953 - val_loss: 0.2951 - val_accuracy: 0.8975\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2974 - accuracy: 0.8954 - val_loss: 0.2944 - val_accuracy: 0.8963\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2979 - accuracy: 0.8955 - val_loss: 0.2932 - val_accuracy: 0.8967\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2971 - accuracy: 0.8955 - val_loss: 0.2888 - val_accuracy: 0.8991\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2962 - accuracy: 0.8959 - val_loss: 0.2917 - val_accuracy: 0.8971\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2970 - accuracy: 0.8961 - val_loss: 0.2937 - val_accuracy: 0.8979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9GxZE5Yntpw"
      },
      "source": [
        "def evaluate_attack(attack_model, X_attack, y_attack, n_classes):\n",
        "  acc_per_class = []\n",
        "  for c in range(n_classes):\n",
        "    class_instances = X_attack[:, 0] == c # get same class samples\n",
        "    test_loss, test_acc = attack_model.evaluate(X_attack[class_instances, :], y_attack[class_instances], verbose=0)\n",
        "    acc_per_class.append(test_acc)\n",
        "    print(f\"class-{c+1}: {test_acc}\")\n",
        "  return acc_per_class\n",
        "\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQyYdB7SABg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b3bc5b-f499-4172-f32c-1cb734fd8b11"
      },
      "source": [
        "# create a test dataset \n",
        "\n",
        "D_out = prepare_batch(target_model, test_images, test_labels, False)\n",
        "D_in = prepare_batch(target_model, train_images[:10000], train_labels[:10000], True)\n",
        "print(\"Testing with 'in' data only:\")\n",
        "res_in = evaluate_attack(attack_model, D_in[:, :-1], D_in[:, -1], 10)\n",
        "\n",
        "print(\"\\nTesting with 'out' data only:\")\n",
        "res_out = evaluate_attack(attack_model, D_out[:, :-1], D_out[:, -1], 10)\n",
        "\n",
        "print(\"\\nTesting with all prev data: \")\n",
        "res_all = evaluate_attack(attack_model, np.concatenate((D_out[:, :-1], D_in[:, :-1])), np.concatenate((D_out[:, -1], D_in[:, -1])), 10)\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with 'in' data only:\n",
            "class-1: 0.7273631691932678\n",
            "class-2: 0.8675564527511597\n",
            "class-3: 0.5784883499145508\n",
            "class-4: 0.6072834730148315\n",
            "class-5: 0.8208208084106445\n",
            "class-6: 0.8121665120124817\n",
            "class-7: 0.9067960977554321\n",
            "class-8: 0.8091908097267151\n",
            "class-9: 0.8936585187911987\n",
            "class-10: 0.8725789785385132\n",
            "\n",
            "Testing with 'out' data only:\n",
            "class-1: 0.5350000262260437\n",
            "class-2: 0.35199999809265137\n",
            "class-3: 0.6869999766349792\n",
            "class-4: 0.7279999852180481\n",
            "class-5: 0.5299999713897705\n",
            "class-6: 0.5139999985694885\n",
            "class-7: 0.27900001406669617\n",
            "class-8: 0.43799999356269836\n",
            "class-9: 0.3059999942779541\n",
            "class-10: 0.38999998569488525\n",
            "\n",
            "Testing with all prev data: \n",
            "class-1: 0.6314214468002319\n",
            "class-2: 0.6063829660415649\n",
            "class-3: 0.6318897604942322\n",
            "class-4: 0.6671627163887024\n",
            "class-5: 0.6753376722335815\n",
            "class-6: 0.6582343578338623\n",
            "class-7: 0.5975369215011597\n",
            "class-8: 0.6236881613731384\n",
            "class-9: 0.6034567952156067\n",
            "class-10: 0.6289752721786499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYRgNm8sqkY7"
      },
      "source": [
        "attack_model_bundle"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}