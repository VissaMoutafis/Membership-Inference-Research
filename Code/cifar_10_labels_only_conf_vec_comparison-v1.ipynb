{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vzAuc5Xo3j2"
      },
      "source": [
        "# Label Only Membership Inference (Revisited on points)\n",
        "\n",
        "### Threat Model:\n",
        "\n",
        "- **Black Box** access to an overfitted classifier with no access to actual $D_{train}$\n",
        "- Predict API returns **only labels instead of confidence vectors**\n",
        "- We have some insight on the training data distribution, $D_{out}$ , **but** $D_{train} \\cap D_{out} = \\varnothing$\n",
        "\n",
        "\n",
        "### Attack Target: \n",
        "- Use a shadow model to attack local shadow models and extract membership leakage features\n",
        "- Use data perturbations in order to exploit test/training data approximation relevancies to the classification boundaries.\n",
        "- Perfom the boundary-based attack on the actual model\n",
        "\n",
        "### Evaluation Target\n",
        "- Score over $50\\%$ accuracy\n",
        "- Train attack model based on this assumption and compare with conf-vector attack\n",
        "\n",
        "Implemented based on [this paper](https://arxiv.org/abs/2007.14321)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg_WQJ7j5n3B",
        "outputId": "01f7f8f3-6f4a-4551-cc79-379d2853b465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# for image interpolation\n",
        "import scipy.ndimage.interpolation as interpolation\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from mia.attack_model import *\n",
        "from mia.label_only import *\n",
        "from mia.shadow_models import *\n",
        "from mia.utilities import *\n",
        "from mia.wrappers import ConfidenceVectorAttack\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kv7Qon0qRTk"
      },
      "source": [
        "## Target Model\n",
        "\n",
        "### Model Architecture\n",
        "- 2 layers of 32 $3\\times 3$ Conv2D filters with Max Pooling\n",
        "- 2 layers of 64 $3\\times 3$ Conv2D filters with MaxPooling\n",
        "- Dense Layer of 512 neurons\n",
        "- Dense Output layer of 10 neurons\n",
        "- Each layer has ReLU activation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zacp4ArauIET"
      },
      "outputs": [],
      "source": [
        "D_TARGET_SIZE = 2500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1qg7LkXX0zOF"
      },
      "outputs": [],
      "source": [
        "def f_target(X_train, y_train, X_test=None, y_test=None, epochs=100):\n",
        "  \"\"\"\n",
        "  Returns a trained target model, if test data are specified we will evaluate the model and print its accuracy\n",
        "  \"\"\"\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(32, activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(10))\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  if X_test is None or y_test is None:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_split=0.2)\n",
        "  else:\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                    validation_data=(X_test, y_test))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oy2NLipP75sX"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "train_images = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images=test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "#shuffle the datasets\n",
        "sample_i = np.random.choice(range(train_images.shape[0]), train_images.shape[0], replace=False)\n",
        "train_images = train_images[sample_i]\n",
        "train_labels = train_labels[sample_i]\n",
        "sample_i = np.random.choice(range(test_images.shape[0]), test_images.shape[0], replace=False)\n",
        "test_images = test_images[sample_i]\n",
        "test_labels = test_labels[sample_i] \n",
        "\n",
        "# define the target's training dataset and the attacker's retrieved dataset\n",
        "attacker_labels = np.concatenate((train_labels[D_TARGET_SIZE:], test_labels))\n",
        "attacker_images = np.concatenate((train_images[D_TARGET_SIZE:], test_images))\n",
        "target_images = train_images[:D_TARGET_SIZE]\n",
        "target_labels = train_labels[:D_TARGET_SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap02yKRT76RJ",
        "outputId": "193e0b03-008d-4053-d8ad-77d1ed10a427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "63/63 [==============================] - 3s 18ms/step - loss: 3.6304 - accuracy: 0.3550 - val_loss: 1.2286 - val_accuracy: 0.5420\n",
            "Epoch 2/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 1.1157 - accuracy: 0.6240 - val_loss: 0.8881 - val_accuracy: 0.7200\n",
            "Epoch 3/25\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.6742 - accuracy: 0.7800 - val_loss: 0.6676 - val_accuracy: 0.7680\n",
            "Epoch 4/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.4118 - accuracy: 0.8680 - val_loss: 0.4747 - val_accuracy: 0.8740\n",
            "Epoch 5/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.2195 - accuracy: 0.9355 - val_loss: 0.5100 - val_accuracy: 0.8620\n",
            "Epoch 6/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.1477 - accuracy: 0.9565 - val_loss: 0.3643 - val_accuracy: 0.9100\n",
            "Epoch 7/25\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0948 - accuracy: 0.9700 - val_loss: 0.3596 - val_accuracy: 0.9220\n",
            "Epoch 8/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0607 - accuracy: 0.9805 - val_loss: 0.3677 - val_accuracy: 0.9240\n",
            "Epoch 9/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0505 - accuracy: 0.9875 - val_loss: 0.3519 - val_accuracy: 0.9260\n",
            "Epoch 10/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0408 - accuracy: 0.9900 - val_loss: 0.3477 - val_accuracy: 0.9320\n",
            "Epoch 11/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0388 - accuracy: 0.9920 - val_loss: 0.3683 - val_accuracy: 0.9300\n",
            "Epoch 12/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0262 - accuracy: 0.9930 - val_loss: 0.3458 - val_accuracy: 0.9320\n",
            "Epoch 13/25\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0246 - accuracy: 0.9930 - val_loss: 0.3402 - val_accuracy: 0.9380\n",
            "Epoch 14/25\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.3524 - val_accuracy: 0.9340\n",
            "Epoch 15/25\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0234 - accuracy: 0.9935 - val_loss: 0.3752 - val_accuracy: 0.9260\n",
            "Epoch 16/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0614 - accuracy: 0.9850 - val_loss: 0.5909 - val_accuracy: 0.8940\n",
            "Epoch 17/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.1062 - accuracy: 0.9720 - val_loss: 0.4738 - val_accuracy: 0.9080\n",
            "Epoch 18/25\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0736 - accuracy: 0.9795 - val_loss: 0.4914 - val_accuracy: 0.9200\n",
            "Epoch 19/25\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0401 - accuracy: 0.9880 - val_loss: 0.3954 - val_accuracy: 0.9200\n",
            "Epoch 20/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9910 - val_loss: 0.3555 - val_accuracy: 0.9380\n",
            "Epoch 21/25\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0269 - accuracy: 0.9915 - val_loss: 0.3761 - val_accuracy: 0.9380\n",
            "Epoch 22/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 0.4439 - val_accuracy: 0.9320\n",
            "Epoch 23/25\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.4454 - val_accuracy: 0.9300\n",
            "Epoch 24/25\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0285 - accuracy: 0.9925 - val_loss: 0.4332 - val_accuracy: 0.9240\n",
            "Epoch 25/25\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 0.4099 - val_accuracy: 0.9300\n"
          ]
        }
      ],
      "source": [
        "train_images, eval_images, train_labels, eval_labels = train_test_split(target_images, target_labels, test_size=0.2, shuffle=True)\n",
        "target_model = f_target(train_images, train_labels, eval_images, eval_labels, epochs=25) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2GWyCXmmwIiJ"
      },
      "outputs": [],
      "source": [
        "N_SHADOWS = 5\n",
        "D_SHADOW_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2rwhySHfVQjV"
      },
      "outputs": [],
      "source": [
        "def f_shadow():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  \n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='tanh'))\n",
        "\n",
        "  model.add(layers.Dense(10))\n",
        "  \n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack = ConfidenceVectorAttack(target_model, (train_images, train_labels), (attacker_images, attacker_labels), f_shadow, \n",
        "                                n_shadows=N_SHADOWS, D_shadow_size=D_SHADOW_SIZE)\n",
        "attack.perform_attack()\n",
        "attack.evaluate_attack()"
      ],
      "metadata": {
        "id": "229vDaGl1kKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0763e5c3-43c7-4289-f460-ec8bd3a82076"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 15ms/step - loss: 1.2791 - accuracy: 0.5955 - val_loss: 0.6485 - val_accuracy: 0.8152\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.3765 - accuracy: 0.9045 - val_loss: 0.4533 - val_accuracy: 0.8424\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.1933 - accuracy: 0.9657 - val_loss: 0.3908 - val_accuracy: 0.8818\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.1087 - accuracy: 0.9896 - val_loss: 0.3606 - val_accuracy: 0.8879\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0660 - accuracy: 0.9955 - val_loss: 0.3447 - val_accuracy: 0.8848\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.3456 - val_accuracy: 0.9000\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.8909\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 0.9000\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.3241 - val_accuracy: 0.8970\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.3196 - val_accuracy: 0.9030\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.3215 - val_accuracy: 0.9030\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.9000\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3194 - val_accuracy: 0.8970\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.8970\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 0.9030\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.8970\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3205 - val_accuracy: 0.9030\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9030\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9000\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.9030\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9000\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3211 - val_accuracy: 0.9030\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 0.9030\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3212 - val_accuracy: 0.9030\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 0.9030\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 15ms/step - loss: 1.0606 - accuracy: 0.6522 - val_loss: 0.6221 - val_accuracy: 0.8091\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.2447 - accuracy: 0.9433 - val_loss: 0.4698 - val_accuracy: 0.8848\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.1016 - accuracy: 0.9910 - val_loss: 0.4078 - val_accuracy: 0.8970\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.3833 - val_accuracy: 0.8939\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9030\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 0.9061\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.3697 - val_accuracy: 0.9091\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9091\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 0.9121\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 0.9121\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 0.9091\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9121\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.3758 - val_accuracy: 0.9091\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.9121\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3776 - val_accuracy: 0.9091\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.3769 - val_accuracy: 0.9091\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.9091\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 0.9091\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 10ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.9121\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.9121\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 0.9121\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3838 - val_accuracy: 0.9121\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3810 - val_accuracy: 0.9152\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3821 - val_accuracy: 0.9152\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3847 - val_accuracy: 0.9152\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 17ms/step - loss: 1.3960 - accuracy: 0.5955 - val_loss: 0.7714 - val_accuracy: 0.7788\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.3909 - accuracy: 0.9090 - val_loss: 0.6136 - val_accuracy: 0.8242\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.2054 - accuracy: 0.9672 - val_loss: 0.5921 - val_accuracy: 0.8364\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.1124 - accuracy: 0.9836 - val_loss: 0.5561 - val_accuracy: 0.8364\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0682 - accuracy: 0.9955 - val_loss: 0.5207 - val_accuracy: 0.8545\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.5329 - val_accuracy: 0.8424\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.8303\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.5183 - val_accuracy: 0.8364\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.5204 - val_accuracy: 0.8364\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.5174 - val_accuracy: 0.8273\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8333\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.5119 - val_accuracy: 0.8364\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.5154 - val_accuracy: 0.8333\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.8364\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.8364\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.8424\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8333\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.5094 - val_accuracy: 0.8455\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.5093 - val_accuracy: 0.8424\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.5101 - val_accuracy: 0.8485\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.8455\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5139 - val_accuracy: 0.8424\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.8424\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8455\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.8424\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 15ms/step - loss: 1.1767 - accuracy: 0.6239 - val_loss: 0.6341 - val_accuracy: 0.8212\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.3310 - accuracy: 0.9149 - val_loss: 0.4979 - val_accuracy: 0.8576\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.1521 - accuracy: 0.9761 - val_loss: 0.4113 - val_accuracy: 0.8848\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0851 - accuracy: 0.9955 - val_loss: 0.3791 - val_accuracy: 0.8818\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.3729 - val_accuracy: 0.8939\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.3603 - val_accuracy: 0.8939\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.8939\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.3550 - val_accuracy: 0.9000\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9000\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9000\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.3502 - val_accuracy: 0.9000\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.9030\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9030\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9000\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 0.9030\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3485 - val_accuracy: 0.9091\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 0.9000\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.9061\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.9061\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9091\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9061\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9030\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9091\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 0.9061\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9061\n",
            "Epoch 1/25\n",
            "21/21 [==============================] - 1s 16ms/step - loss: 1.0401 - accuracy: 0.6627 - val_loss: 0.5745 - val_accuracy: 0.8000\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.2729 - accuracy: 0.9403 - val_loss: 0.4485 - val_accuracy: 0.8606\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9866 - val_loss: 0.3741 - val_accuracy: 0.8697\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0581 - accuracy: 0.9955 - val_loss: 0.3562 - val_accuracy: 0.8879\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0339 - accuracy: 0.9985 - val_loss: 0.3535 - val_accuracy: 0.8970\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.8939\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.9061\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9030\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9091\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.3423 - val_accuracy: 0.9061\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9091\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 0.9061\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9091\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 0.9061\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9091\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 0.9091\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3378 - val_accuracy: 0.9091\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9091\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 0s 9ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9121\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9121\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3378 - val_accuracy: 0.9091\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9091\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9091\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9121\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3367 - val_accuracy: 0.9091\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Preparing shadow batch of size 660\n",
            "Done!\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 1s 8ms/step - loss: 0.7127 - accuracy: 0.4848 - val_loss: 0.6966 - val_accuracy: 0.4980\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6998 - accuracy: 0.4974 - val_loss: 0.6955 - val_accuracy: 0.5131\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6971 - accuracy: 0.4970 - val_loss: 0.6929 - val_accuracy: 0.5131\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6963 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5121\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6949 - accuracy: 0.5048 - val_loss: 0.6922 - val_accuracy: 0.5354\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6943 - accuracy: 0.5173 - val_loss: 0.6916 - val_accuracy: 0.5101\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6942 - accuracy: 0.5082 - val_loss: 0.6916 - val_accuracy: 0.5465\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6938 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5495\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6930 - accuracy: 0.5216 - val_loss: 0.6917 - val_accuracy: 0.5394\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5121 - val_loss: 0.6916 - val_accuracy: 0.5495\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6928 - accuracy: 0.5056 - val_loss: 0.6916 - val_accuracy: 0.5424\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6921 - accuracy: 0.5303 - val_loss: 0.6917 - val_accuracy: 0.5444\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5286 - val_loss: 0.6914 - val_accuracy: 0.5404\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5221 - val_loss: 0.6920 - val_accuracy: 0.5343\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5333 - val_loss: 0.6916 - val_accuracy: 0.5444\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5255 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5078 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5091 - val_loss: 0.6916 - val_accuracy: 0.5384\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5225 - val_loss: 0.6915 - val_accuracy: 0.5394\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5165 - val_loss: 0.6918 - val_accuracy: 0.5242\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5091 - val_loss: 0.6927 - val_accuracy: 0.5081\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6921 - val_accuracy: 0.5152\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5212 - val_loss: 0.6916 - val_accuracy: 0.5364\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5251 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5234 - val_loss: 0.6916 - val_accuracy: 0.5303\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5404\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5290 - val_loss: 0.6916 - val_accuracy: 0.5455\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5208 - val_loss: 0.6914 - val_accuracy: 0.5343\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5268 - val_loss: 0.6915 - val_accuracy: 0.5394\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6913 - accuracy: 0.5329 - val_loss: 0.6917 - val_accuracy: 0.5333\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.5139 - val_loss: 0.6919 - val_accuracy: 0.5242\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5342 - val_loss: 0.6924 - val_accuracy: 0.5121\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5212 - val_loss: 0.6925 - val_accuracy: 0.5152\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5437 - val_loss: 0.6945 - val_accuracy: 0.5010\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6928 - val_accuracy: 0.5111\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5117 - val_loss: 0.6916 - val_accuracy: 0.5283\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5247 - val_loss: 0.6915 - val_accuracy: 0.5515\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5316 - val_loss: 0.6940 - val_accuracy: 0.4980\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5281 - val_loss: 0.6916 - val_accuracy: 0.5293\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5303 - val_loss: 0.6930 - val_accuracy: 0.5051\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6905 - accuracy: 0.5420 - val_loss: 0.6916 - val_accuracy: 0.5273\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5420 - val_loss: 0.6913 - val_accuracy: 0.5374\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5255 - val_loss: 0.6913 - val_accuracy: 0.5485\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5221 - val_loss: 0.6913 - val_accuracy: 0.5313\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5234 - val_loss: 0.6915 - val_accuracy: 0.5354\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6905 - accuracy: 0.5312 - val_loss: 0.6925 - val_accuracy: 0.5081\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5329 - val_loss: 0.6941 - val_accuracy: 0.5010\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5290 - val_loss: 0.6920 - val_accuracy: 0.5141\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6904 - accuracy: 0.5329 - val_loss: 0.6914 - val_accuracy: 0.5323\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6905 - accuracy: 0.5338 - val_loss: 0.6929 - val_accuracy: 0.5020\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6904 - accuracy: 0.5247 - val_loss: 0.6930 - val_accuracy: 0.5051\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5320 - val_loss: 0.6909 - val_accuracy: 0.5333\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.5381 - val_loss: 0.6912 - val_accuracy: 0.5273\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5264 - val_loss: 0.6915 - val_accuracy: 0.5293\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6913 - val_accuracy: 0.5283\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5312 - val_loss: 0.6910 - val_accuracy: 0.5566\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6901 - accuracy: 0.5407 - val_loss: 0.6913 - val_accuracy: 0.5253\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5312 - val_loss: 0.6910 - val_accuracy: 0.5424\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5316 - val_loss: 0.6913 - val_accuracy: 0.5182\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5247 - val_loss: 0.6906 - val_accuracy: 0.5374\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5368 - val_loss: 0.6907 - val_accuracy: 0.5374\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6901 - accuracy: 0.5268 - val_loss: 0.6905 - val_accuracy: 0.5354\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5411 - val_loss: 0.6945 - val_accuracy: 0.4970\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6902 - accuracy: 0.5338 - val_loss: 0.6908 - val_accuracy: 0.5343\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5511 - val_loss: 0.6909 - val_accuracy: 0.5313\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5364 - val_loss: 0.6908 - val_accuracy: 0.5323\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6901 - accuracy: 0.5381 - val_loss: 0.6913 - val_accuracy: 0.5172\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5368 - val_loss: 0.6902 - val_accuracy: 0.5333\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.5320 - val_loss: 0.6899 - val_accuracy: 0.5323\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5377 - val_loss: 0.6914 - val_accuracy: 0.5111\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5424 - val_loss: 0.6901 - val_accuracy: 0.5434\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5394 - val_loss: 0.6899 - val_accuracy: 0.5384\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5351 - val_loss: 0.6904 - val_accuracy: 0.5343\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5299 - val_loss: 0.6902 - val_accuracy: 0.5434\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5368 - val_loss: 0.6922 - val_accuracy: 0.5061\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5351 - val_loss: 0.6905 - val_accuracy: 0.5323\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5472 - val_loss: 0.6906 - val_accuracy: 0.5283\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5264 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6897 - accuracy: 0.5411 - val_loss: 0.6897 - val_accuracy: 0.5394\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6896 - accuracy: 0.5325 - val_loss: 0.6909 - val_accuracy: 0.5141\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5355 - val_loss: 0.6897 - val_accuracy: 0.5455\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5325 - val_loss: 0.6908 - val_accuracy: 0.5182\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6894 - accuracy: 0.5481 - val_loss: 0.6908 - val_accuracy: 0.5212\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6893 - accuracy: 0.5346 - val_loss: 0.6906 - val_accuracy: 0.5253\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6894 - accuracy: 0.5416 - val_loss: 0.6904 - val_accuracy: 0.5343\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.5355 - val_loss: 0.6896 - val_accuracy: 0.5273\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5238 - val_loss: 0.6896 - val_accuracy: 0.5404\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5325 - val_loss: 0.6898 - val_accuracy: 0.5364\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6888 - accuracy: 0.5359 - val_loss: 0.6892 - val_accuracy: 0.5293\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5299 - val_loss: 0.6899 - val_accuracy: 0.5263\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.5411 - val_loss: 0.6894 - val_accuracy: 0.5384\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6889 - accuracy: 0.5381 - val_loss: 0.6892 - val_accuracy: 0.5455\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5351 - val_loss: 0.6891 - val_accuracy: 0.5333\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 1s 10ms/step - loss: 0.6890 - accuracy: 0.5394 - val_loss: 0.6908 - val_accuracy: 0.5192\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5446 - val_loss: 0.6890 - val_accuracy: 0.5263\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5342 - val_loss: 0.6893 - val_accuracy: 0.5505\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.5368 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5242 - val_loss: 0.6896 - val_accuracy: 0.5313\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 0s 6ms/step - loss: 0.6885 - accuracy: 0.5498 - val_loss: 0.6892 - val_accuracy: 0.5465\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.6889 - accuracy: 0.5346 - val_loss: 0.6894 - val_accuracy: 0.5263\n",
            "class-1 acc: 0.47668394446372986\n",
            "class-2 acc: 0.5083932876586914\n",
            "class-3 acc: 0.48711341619491577\n",
            "class-4 acc: 0.5166240334510803\n",
            "class-5 acc: 0.512135922908783\n",
            "class-6 acc: 0.49435028433799744\n",
            "class-7 acc: 0.5366430282592773\n",
            "class-8 acc: 0.4944320619106293\n",
            "class-9 acc: 0.572890043258667\n",
            "class-10 acc: 0.4884318709373474\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.41      0.45      2000\n",
            "         1.0       0.51      0.61      0.56      2000\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.50      4000\n",
            "weighted avg       0.51      0.51      0.50      4000\n",
            "\n",
            "AUC: 0.517097125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8grjprA4PmE7"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9g2jus3rP1j"
      },
      "source": [
        "## Check target model's behaviour in perturbed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sD5YbfusfCob"
      },
      "outputs": [],
      "source": [
        "def study_perturbations(model, X, y, rs, ts):\n",
        "  diffs = []\n",
        "  y_pred = target_predict(model, X)    \n",
        "  for c in range(10):\n",
        "    #  given class acquire the changes in perturbed input instances given the model\n",
        "    idx = y_pred[:, 0] == c\n",
        "    X_c = X[idx]\n",
        "    y_pred_c = y_pred[idx]\n",
        "    perturbed_labels = augmented_queries(model, X_c, y_pred_c, rs, ts)\n",
        "    # Now we have to count how many labels diverge from the predicted label\n",
        "    diff = len(perturbed_labels.reshape(-1)) - sum(perturbed_labels.reshape(-1)) # the labels are binary where 1 == y_pred = y_perturbed, otherwise 0\n",
        "    diffs.append(int(100 * diff/len(perturbed_labels.reshape(-1)))) # append the percentage of changes in the class sample\n",
        "    \n",
        "  return diffs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w-0sJVhfbfQ3"
      },
      "outputs": [],
      "source": [
        "N_SAMPLES = 100\n",
        "train_idx = np.random.choice(range(train_images.shape[0]), N_SAMPLES, replace=False)\n",
        "test_idx = np.random.choice(range(attacker_images.shape[0]), N_SAMPLES, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dpYCPdS0EA-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "3ab9aa30-31d0-43c8-9b13-c71e5d7ad8d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6a4b0dea59dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# tests in D_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdiffs_per_class_D_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy_perturbations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtotal_diffs_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs_per_class_D_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D_in predicted label divergence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAEzCAYAAAB0TDEBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATyklEQVR4nO3dbYil51kH8P/VrFGstUqzgmQ3JuLWulahdQgVQSutsomQ/eALCRSthC6+RARFiFSqxE9VVBDiy4IlVbBp7AdZcEtATQkUU7OlNTYpkTVWs1HMWmu/lDYNXn6Yo57MzmTOzp6X5578fjBwnufcO+e6c2b/5L/POWequwMAAMA4XrXpAQAAALg6ihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMZt8iV1Xvq6rnq+pTe9xfVfW7VXWxqp6oqjcvf0yAK8knYIpkE7AOi1yReyDJqZe5/7YkJ2ZfZ5L8/rWPBbCQByKfgOl5ILIJWLF9i1x3P5rkP19myekkf9zbHkvydVX1jcsaEGAv8gmYItkErMMy3iN3Y5Jn544vzc4BbJp8AqZINgHX7Mg6H6yqzmT7JQR59atf/V1veMMb1vnwwIp9/OMf/4/uPrrpOa6WbILDTz4BU3Qt2bSMIvdckuNzx8dm567Q3WeTnE2Sra2tvnDhwhIeHpiKqvrnTc+ww0L5JJvg8JtYPvl/JyDJtWXTMl5aeS7Jj88+gektST7f3f+2hO8LcK3kEzBFsgm4ZvtekauqDyR5a5IbqupSkl9N8hVJ0t1/kOR8ktuTXEzyhSQ/uaphAebJJ2CKZBOwDvsWue6+a5/7O8nPLm0igAXJJ2CKZBOwDst4aSUAAABrpMgBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFipyVXWqqp6uqotVde8u999UVY9U1Seq6omqun35owK8lGwCpko+Aau2b5GrquuS3J/ktiQnk9xVVSd3LPuVJA9195uS3Jnk95Y9KMA82QRMlXwC1mGRK3K3JrnY3c909wtJHkxyeseaTvK1s9uvTfKvyxsRYFeyCZgq+QSs3CJF7sYkz84dX5qdm/drSd5RVZeSnE/yc7t9o6o6U1UXqurC5cuXDzAuwP+RTcBUySdg5Zb1YSd3JXmgu48luT3Jn1TVFd+7u89291Z3bx09enRJDw2wJ9kETJV8Aq7JIkXuuSTH546Pzc7NuzvJQ0nS3X+T5KuS3LCMAQH2IJuAqZJPwMotUuQeT3Kiqm6pquuz/YbcczvW/EuStyVJVX1btsPI9X9glWQTMFXyCVi5fYtcd7+Y5J4kDyf5dLY/YenJqrqvqu6YLfvFJO+qqr9L8oEk7+zuXtXQALIJmCr5BKzDkUUWdff5bL8Rd/7ce+ZuP5Xke5Y7GsDLk03AVMknYNWW9WEnAAAArIkiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwSxU5KrqVFU9XVUXq+rePdb8WFU9VVVPVtWfLndMgCvJJmCq5BOwakf2W1BV1yW5P8kPJLmU5PGqOtfdT82tOZHkl5N8T3d/rqq+YVUDAySyCZgu+QSswyJX5G5NcrG7n+nuF5I8mOT0jjXvSnJ/d38uSbr7+eWOCXAF2QRMlXwCVm6RIndjkmfnji/Nzs17fZLXV9VHq+qxqjq1rAEB9iCbgKmST8DK7fvSyqv4PieSvDXJsSSPVtV3dPd/zS+qqjNJziTJTTfdtKSHBtiTbAKmSj4B12SRK3LPJTk+d3xsdm7epSTnuvvL3f1PSf4h2+H0Et19tru3unvr6NGjB50ZIJFNwHTJJ2DlFilyjyc5UVW3VNX1Se5Mcm7Hmj/P9r8opapuyPbLBZ5Z4pwAO8kmYKrkE7By+xa57n4xyT1JHk7y6SQPdfeTVXVfVd0xW/Zwks9W1VNJHknyS9392VUNDSCbgKmST8A6VHdv5IG3trb6woULG3lsYDWq6uPdvbXpOa6FbILDST4BU3Qt2bTQLwQHAABgOhQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMZqEiV1WnqurpqrpYVfe+zLofrqquqq3ljQiwO9kETJV8AlZt3yJXVdcluT/JbUlOJrmrqk7usu41SX4+yceWPSTATrIJmCr5BKzDIlfkbk1ysbuf6e4XkjyY5PQu6349yXuTfHGJ8wHsRTYBUyWfgJVbpMjdmOTZueNLs3P/p6renOR4d//FEmcDeDmyCZgq+QSs3DV/2ElVvSrJbyf5xQXWnqmqC1V14fLly9f60AB7kk3AVMknYBkWKXLPJTk+d3xsdu5/vSbJG5N8pKo+k+QtSc7t9qbd7j7b3VvdvXX06NGDTw0gm4Dpkk/Ayi1S5B5PcqKqbqmq65PcmeTc/97Z3Z/v7hu6++buvjnJY0nu6O4LK5kYYJtsAqZKPgErt2+R6+4Xk9yT5OEkn07yUHc/WVX3VdUdqx4QYDeyCZgq+QSsw5FFFnX3+STnd5x7zx5r33rtYwHsTzYBUyWfgFW75g87AQAAYL0UOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGMxCRa6qTlXV01V1saru3eX+X6iqp6rqiar6q6r6puWPCvBSsgmYKvkErNq+Ra6qrktyf5LbkpxMcldVndyx7BNJtrr7O5N8KMlvLHtQgHmyCZgq+QSswyJX5G5NcrG7n+nuF5I8mOT0/ILufqS7vzA7fCzJseWOCXAF2QRMlXwCVm6RIndjkmfnji/Nzu3l7iQf3u2OqjpTVReq6sLly5cXnxLgSrIJmCr5BKzcUj/spKrekWQryW/udn93n+3ure7eOnr06DIfGmBPsgmYKvkEHNSRBdY8l+T43PGx2bmXqKq3J3l3ku/r7i8tZzyAPckmYKrkE7Byi1yRezzJiaq6paquT3JnknPzC6rqTUn+MMkd3f388scEuIJsAqZKPgErt2+R6+4Xk9yT5OEkn07yUHc/WVX3VdUds2W/meRrkvxZVX2yqs7t8e0AlkI2AVMln4B1WOSllenu80nO7zj3nrnbb1/yXAD7kk3AVMknYNWW+mEnAAAArJ4iBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwSxU5KrqVFU9XVUXq+reXe7/yqr64Oz+j1XVzcseFGAn2QRMlXwCVm3fIldV1yW5P8ltSU4muauqTu5YdneSz3X3tyT5nSTvXfagAPNkEzBV8glYh0WuyN2a5GJ3P9PdLyR5MMnpHWtOJ3n/7PaHkrytqmp5YwJcQTYBUyWfgJVbpMjdmOTZueNLs3O7runuF5N8PsnrljEgwB5kEzBV8glYuSPrfLCqOpPkzOzwS1X1qXU+/grckOQ/Nj3EEhyGfdjDNHzrpgc4iEOYTcnh+Hmyh2k4DHtI5NNUHIafp8Owh+Rw7OMw7OHA2bRIkXsuyfG542Ozc7utuVRVR5K8Nslnd36j7j6b5GySVNWF7t46yNBTcRj2kByOfdjDNFTVhTU+nGx6GYdhH/YwDYdhD4l8mgp7mI7DsI/DsoeD/tlFXlr5eJITVXVLVV2f5M4k53asOZfkJ2a3fyTJX3d3H3QogAXIJmCq5BOwcvtekevuF6vqniQPJ7kuyfu6+8mqui/Jhe4+l+SPkvxJVV1M8p/ZDiyAlZFNwFTJJ2AdFnqPXHefT3J+x7n3zN3+YpIfvcrHPnuV66foMOwhORz7sIdpWOseZNPLOgz7sIdpOAx7SOTTVNjDdByGfbyi91Cu4gMAAIxlkffIAQAAMCErL3JVdaqqnq6qi1V17y73f2VVfXB2/8eq6uZVz3S1FtjDL1TVU1X1RFX9VVV90ybmfDn77WFu3Q9XVVfV5D4BaJE9VNWPzZ6LJ6vqT9c94yIW+Hm6qaoeqapPzH6mbt/EnHupqvdV1fN7fQR2bfvd2f6eqKo3r3vGRcim6ZBP0zB6NiXyaUoOQz7JpukYPZ9Wlk3dvbKvbL/B9x+TfHOS65P8XZKTO9b8TJI/mN2+M8kHVznTivbw/Um+enb7p0fcw2zda5I8muSxJFubnvsAz8OJJJ9I8vWz42/Y9NwH3MfZJD89u30yyWc2PfeO+b43yZuTfGqP+29P8uEkleQtST626ZkP+DzIponsY7ZOPm1+D5POptlc8mkCX4chn2TTdL4OQz6tKptWfUXu1iQXu/uZ7n4hyYNJTu9YczrJ+2e3P5TkbVVVK57rauy7h+5+pLu/MDt8LNu/L2ZKFnkekuTXk7w3yRfXOdyCFtnDu5Lc392fS5Lufn7NMy5ikX10kq+d3X5tkn9d43z76u5Hs/0Ja3s5neSPe9tjSb6uqr5xPdMtTDZNh3yahuGzKZFPa5xxP4chn2TTdAyfT6vKplUXuRuTPDt3fGl2btc13f1iks8ned2K57oai+xh3t3ZbtRTsu8eZpdwj3f3X6xzsKuwyPPw+iSvr6qPVtVjVXVqbdMtbpF9/FqSd1TVpWx/4tnPrWe0pbnavzObIJumQz5NwyshmxL5tC6HIZ9k03S8EvLpQNm00K8fYDFV9Y4kW0m+b9OzXI2qelWS307yzg2Pcq2OZPslAm/N9r/sPVpV39Hd/7XRqa7eXUke6O7fqqrvzvbvGXpjd//3pgdjTKNmUyKfJkY2sXSj5pNsmpxXZD6t+orcc0mOzx0fm53bdU1VHcn25dDPrniuq7HIHlJVb0/y7iR3dPeX1jTbovbbw2uSvDHJR6rqM9l+be65ib1pd5Hn4VKSc9395e7+pyT/kO1wmpJF9nF3koeSpLv/JslXJblhLdMtx0J/ZzZMNk2HfJqGV0I2JfJpXQ5DPsmm6Xgl5NPBsmnFb+w7kuSZJLfk/9+c+O071vxsXvqG3YdWOdOK9vCmbL8J88Sm5z3oHnas/0im94bdRZ6HU0neP7t9Q7YvUb9u07MfYB8fTvLO2e1vy/brvGvTs++Y8ebs/YbdH8pL37D7t5ue94DPg2yayD52rJdPm9vD5LNpNpt8GmMPk84n2bT5+a9yH5PPp1Vk0zqGvj3b7f4fk7x7du6+bP/rS7LdmP8sycUkf5vkmzf9H/oAe/jLJP+e5JOzr3Obnvlq97Bj7eTCaMHnobL9Moenkvx9kjs3PfMB93EyyUdnQfXJJD+46Zl3zP+BJP+W5MvZ/pe8u5P8VJKfmnse7p/t7++n+LO04PMgmyayjx1r5dPm9jDpbJrNKJ8m8nUY8kk2Tedr9HxaVTbV7A8DAAAwiJX/QnAAAACWS5EDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABvM/2mZZMj1UMOYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# tests in D_in\n",
        "diffs_per_class_D_in = study_perturbations(target_model, train_images[train_idx], train_labels[train_idx], r, d) \n",
        "total_diffs_in = sum(diffs_per_class_D_in)/10\n",
        "axes[0].set_title('D_in predicted label divergence');\n",
        "axes[0].barh(list(range(10)), diffs_per_class_D_in, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "# tests in D_out  \n",
        "diffs_per_class_D_out= study_perturbations(target_model,attacker_images[test_idx],attacker_labels[test_idx], r, d)\n",
        "total_diffs_out = sum(diffs_per_class_D_out)/10\n",
        "axes[1].set_title('D_out predicted label divergence');\n",
        "axes[1].barh(list(range(10)), diffs_per_class_D_out, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "axes[2].set_title('Total predicted label divergence percentage');\n",
        "axes[2].barh([1, 0], [total_diffs_in, total_diffs_out], tick_label=['In', 'Out'])\n",
        "\n",
        "plt.setp(axes, xticks=range(0, 101, 10), xticklabels=[f'{i}%' for i in range(0, 101, 10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that $x \\in D_{target}$ are more robust to perturbations, than any other instance not in target's training dataset. Nevertheless, whether the model is overfitted or not plays an important role to this training feature, meaning that the more overfit the model is the easier it gets to infer membership of tested instances. "
      ],
      "metadata": {
        "id": "nfA8cQ8mAioZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZOpEdcmCMXX"
      },
      "source": [
        "## Attack a perturbation trained model\n",
        "\n",
        "- We will apply augmentations to the dataset and re-train the target model.\n",
        "- The attacker **does not** know our augmentation settings, so he will train with a normal dataset of zero augmentations\n",
        "- We want to measure the quality of the attack when the target tries to defend MIAs by adding perturbed images of data samples "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cl0sR-1I_sG"
      },
      "outputs": [],
      "source": [
        "# We will defend against the same rotations and translations that the attack models uses (worst case for the attacker)\n",
        "rotates = create_rotates(r)\n",
        "translates = create_translates(d)\n",
        "\n",
        "X_train_aug = train_images\n",
        "X_eval_aug = eval_images\n",
        "y_train_aug = np.concatenate(tuple([train_labels] + [train_labels for rot in rotates] + [train_labels for tra in translates]))\n",
        "y_eval_aug = np.concatenate(tuple([eval_labels] + [eval_labels for rot in rotates] + [eval_labels for tra in translates]))\n",
        "\n",
        "\n",
        "\n",
        "for rot in rotates:\n",
        "  aug_x = apply_augment(train_images, rot, 'r')\n",
        "  X_train_aug = np.concatenate((X_train_aug,aug_x))\n",
        "  aug_x = apply_augment(eval_images, rot, 'r')\n",
        "  X_eval_aug = np.concatenate((X_eval_aug,aug_x))\n",
        "\n",
        "for tra in translates:\n",
        "  aug_x = apply_augment(train_images, tra, 'd')\n",
        "  X_train_aug = np.concatenate((X_train_aug,aug_x))\n",
        "  aug_x = apply_augment(eval_images, tra, 'd')\n",
        "  X_eval_aug = np.concatenate((X_eval_aug ,aug_x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaNvyxztuwED"
      },
      "outputs": [],
      "source": [
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  X_train_aug = tf.convert_to_tensor(X_train_aug)\n",
        "  y_train_aug = tf.convert_to_tensor(y_train_aug)\n",
        "  X_eval_aug = tf.convert_to_tensor(X_eval_aug)\n",
        "  y_eval_aug = tf.convert_to_tensor(y_eval_aug)\n",
        "  target_model = f_target(X_train_aug, y_train_aug, X_eval_aug, y_eval_aug, epochs=10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRYuPrCUBWl"
      },
      "source": [
        "The model is quite overfitted so now all that is left is to evaluate the attack model we created before on the newly trained and \"defended\" target model with perturbations in the train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyt1sD06SBFa"
      },
      "outputs": [],
      "source": [
        "D_in = attack_model.prepare_batch(target_model, train_images[:1000], train_labels[:1000], True)\n",
        "\n",
        "D_out = attack_model.prepare_batch(target_model, attacker_images[:1000], attacker_labels[:1000], False)\n",
        "\n",
        "attack_model.evaluate(np.concatenate((D_out[:, :-1], D_in[:, :-1])),  np.concatenate((D_out[:, -1], D_in[:, -1])), 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW_JINmBUgd_"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "To conclude if the model is more vulnerable, we must meassure the label divergence percentage in the adjusted-to-augmentations model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWDU6N1uSCnA"
      },
      "outputs": [],
      "source": [
        "# test it onthe same data as we tested the non-adjusted to augmentation model\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# tests in D_in\n",
        "diffs_per_class_D_in = study_perturbations(target_model, train_images[train_idx], train_labels[train_idx], r, d) \n",
        "total_diffs_in = sum(diffs_per_class_D_in)/10\n",
        "axes[0].set_title('D_in predicted label divergence');\n",
        "axes[0].barh(list(range(10)), diffs_per_class_D_in, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "# tests in D_out  \n",
        "diffs_per_class_D_out= study_perturbations(target_model,attacker_images[test_idx],attacker_labels[test_idx], r, d)\n",
        "total_diffs_out = sum(diffs_per_class_D_out)/10\n",
        "axes[1].set_title('D_out predicted label divergence');\n",
        "axes[1].barh(list(range(10)), diffs_per_class_D_out, tick_label=[f'Class-{i}' for i in range(10)])\n",
        "\n",
        "axes[2].set_title('Total predicted label divergence percentage');\n",
        "axes[2].barh([1, 0], [total_diffs_in, total_diffs_out], tick_label=['In', 'Out'])\n",
        "\n",
        "plt.setp(axes, xticks=range(0, 101, 10), xticklabels=[f'{i}%' for i in range(0, 101, 10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyVo6yukPGH"
      },
      "source": [
        "We can see that the general percentage of predicted label divergence has fallen, **but** the confidence of the ML algorithm in predicting the label of perturbed instances of instances in $D_{target}$ is better that before and that is realized by the increase of the AUC value (~0.52 when before ~0.51). This means that the adjusted model is even more vulnerable and our attack predicts membership with high sensitivity to the predicted label changes. Next step is to run all with some defences on."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dCUaNVSgL1X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xO89rNuPMca4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cifar_10_labels_only_conf-vec-comparison-v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}